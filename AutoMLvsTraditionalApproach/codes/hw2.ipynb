{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import balanced_accuracy_score,f1_score\n",
    "from supervised.automl import AutoML\n",
    "from sklearn.feature_selection import RFECV\n",
    "# from autogluon.tabular import  TabularPredictor ##with Python 3.9.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/artificial_train.data', delim_whitespace=True, header=None)\n",
    "y = pd.read_csv(\"data/artificial_train.labels\", header=None)\n",
    "Xtest = pd.read_csv('data/artificial_test.data', delim_whitespace=True, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>477</td>\n",
       "      <td>537</td>\n",
       "      <td>479</td>\n",
       "      <td>452</td>\n",
       "      <td>471</td>\n",
       "      <td>491</td>\n",
       "      <td>476</td>\n",
       "      <td>475</td>\n",
       "      <td>473</td>\n",
       "      <td>...</td>\n",
       "      <td>477</td>\n",
       "      <td>481</td>\n",
       "      <td>477</td>\n",
       "      <td>485</td>\n",
       "      <td>511</td>\n",
       "      <td>485</td>\n",
       "      <td>481</td>\n",
       "      <td>479</td>\n",
       "      <td>475</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483</td>\n",
       "      <td>458</td>\n",
       "      <td>460</td>\n",
       "      <td>487</td>\n",
       "      <td>587</td>\n",
       "      <td>475</td>\n",
       "      <td>526</td>\n",
       "      <td>479</td>\n",
       "      <td>485</td>\n",
       "      <td>469</td>\n",
       "      <td>...</td>\n",
       "      <td>463</td>\n",
       "      <td>478</td>\n",
       "      <td>487</td>\n",
       "      <td>338</td>\n",
       "      <td>513</td>\n",
       "      <td>486</td>\n",
       "      <td>483</td>\n",
       "      <td>492</td>\n",
       "      <td>510</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>487</td>\n",
       "      <td>542</td>\n",
       "      <td>499</td>\n",
       "      <td>468</td>\n",
       "      <td>448</td>\n",
       "      <td>471</td>\n",
       "      <td>442</td>\n",
       "      <td>478</td>\n",
       "      <td>480</td>\n",
       "      <td>477</td>\n",
       "      <td>...</td>\n",
       "      <td>487</td>\n",
       "      <td>481</td>\n",
       "      <td>492</td>\n",
       "      <td>650</td>\n",
       "      <td>506</td>\n",
       "      <td>501</td>\n",
       "      <td>480</td>\n",
       "      <td>489</td>\n",
       "      <td>499</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480</td>\n",
       "      <td>491</td>\n",
       "      <td>510</td>\n",
       "      <td>485</td>\n",
       "      <td>495</td>\n",
       "      <td>472</td>\n",
       "      <td>417</td>\n",
       "      <td>474</td>\n",
       "      <td>502</td>\n",
       "      <td>476</td>\n",
       "      <td>...</td>\n",
       "      <td>491</td>\n",
       "      <td>480</td>\n",
       "      <td>474</td>\n",
       "      <td>572</td>\n",
       "      <td>454</td>\n",
       "      <td>469</td>\n",
       "      <td>475</td>\n",
       "      <td>482</td>\n",
       "      <td>494</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>484</td>\n",
       "      <td>502</td>\n",
       "      <td>528</td>\n",
       "      <td>489</td>\n",
       "      <td>466</td>\n",
       "      <td>481</td>\n",
       "      <td>402</td>\n",
       "      <td>478</td>\n",
       "      <td>487</td>\n",
       "      <td>468</td>\n",
       "      <td>...</td>\n",
       "      <td>488</td>\n",
       "      <td>479</td>\n",
       "      <td>452</td>\n",
       "      <td>435</td>\n",
       "      <td>486</td>\n",
       "      <td>508</td>\n",
       "      <td>481</td>\n",
       "      <td>504</td>\n",
       "      <td>495</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>490</td>\n",
       "      <td>505</td>\n",
       "      <td>503</td>\n",
       "      <td>474</td>\n",
       "      <td>463</td>\n",
       "      <td>461</td>\n",
       "      <td>519</td>\n",
       "      <td>476</td>\n",
       "      <td>518</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>467</td>\n",
       "      <td>479</td>\n",
       "      <td>449</td>\n",
       "      <td>588</td>\n",
       "      <td>499</td>\n",
       "      <td>506</td>\n",
       "      <td>475</td>\n",
       "      <td>463</td>\n",
       "      <td>507</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>480</td>\n",
       "      <td>475</td>\n",
       "      <td>476</td>\n",
       "      <td>480</td>\n",
       "      <td>495</td>\n",
       "      <td>482</td>\n",
       "      <td>515</td>\n",
       "      <td>479</td>\n",
       "      <td>480</td>\n",
       "      <td>484</td>\n",
       "      <td>...</td>\n",
       "      <td>464</td>\n",
       "      <td>474</td>\n",
       "      <td>473</td>\n",
       "      <td>424</td>\n",
       "      <td>454</td>\n",
       "      <td>570</td>\n",
       "      <td>476</td>\n",
       "      <td>493</td>\n",
       "      <td>465</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>480</td>\n",
       "      <td>517</td>\n",
       "      <td>631</td>\n",
       "      <td>470</td>\n",
       "      <td>485</td>\n",
       "      <td>474</td>\n",
       "      <td>535</td>\n",
       "      <td>476</td>\n",
       "      <td>493</td>\n",
       "      <td>466</td>\n",
       "      <td>...</td>\n",
       "      <td>501</td>\n",
       "      <td>483</td>\n",
       "      <td>479</td>\n",
       "      <td>687</td>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "      <td>483</td>\n",
       "      <td>500</td>\n",
       "      <td>523</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>484</td>\n",
       "      <td>481</td>\n",
       "      <td>505</td>\n",
       "      <td>478</td>\n",
       "      <td>542</td>\n",
       "      <td>477</td>\n",
       "      <td>518</td>\n",
       "      <td>477</td>\n",
       "      <td>510</td>\n",
       "      <td>472</td>\n",
       "      <td>...</td>\n",
       "      <td>487</td>\n",
       "      <td>483</td>\n",
       "      <td>526</td>\n",
       "      <td>750</td>\n",
       "      <td>486</td>\n",
       "      <td>529</td>\n",
       "      <td>484</td>\n",
       "      <td>473</td>\n",
       "      <td>527</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>474</td>\n",
       "      <td>493</td>\n",
       "      <td>469</td>\n",
       "      <td>486</td>\n",
       "      <td>521</td>\n",
       "      <td>475</td>\n",
       "      <td>494</td>\n",
       "      <td>479</td>\n",
       "      <td>481</td>\n",
       "      <td>473</td>\n",
       "      <td>...</td>\n",
       "      <td>467</td>\n",
       "      <td>476</td>\n",
       "      <td>508</td>\n",
       "      <td>449</td>\n",
       "      <td>463</td>\n",
       "      <td>533</td>\n",
       "      <td>481</td>\n",
       "      <td>489</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n",
       "0     485  477  537  479  452  471  491  476  475  473  ...  477  481  477   \n",
       "1     483  458  460  487  587  475  526  479  485  469  ...  463  478  487   \n",
       "2     487  542  499  468  448  471  442  478  480  477  ...  487  481  492   \n",
       "3     480  491  510  485  495  472  417  474  502  476  ...  491  480  474   \n",
       "4     484  502  528  489  466  481  402  478  487  468  ...  488  479  452   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1995  490  505  503  474  463  461  519  476  518  467  ...  467  479  449   \n",
       "1996  480  475  476  480  495  482  515  479  480  484  ...  464  474  473   \n",
       "1997  480  517  631  470  485  474  535  476  493  466  ...  501  483  479   \n",
       "1998  484  481  505  478  542  477  518  477  510  472  ...  487  483  526   \n",
       "1999  474  493  469  486  521  475  494  479  481  473  ...  467  476  508   \n",
       "\n",
       "      493  494  495  496  497  498  499  \n",
       "0     485  511  485  481  479  475  496  \n",
       "1     338  513  486  483  492  510  517  \n",
       "2     650  506  501  480  489  499  498  \n",
       "3     572  454  469  475  482  494  461  \n",
       "4     435  486  508  481  504  495  511  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "1995  588  499  506  475  463  507  501  \n",
       "1996  424  454  570  476  493  465  485  \n",
       "1997  687  488  488  483  500  523  481  \n",
       "1998  750  486  529  484  473  527  485  \n",
       "1999  449  463  533  481  489  516  516  \n",
       "\n",
       "[2000 rows x 500 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0    -1\n",
       "1    -1\n",
       "2    -1\n",
       "3     1\n",
       "4     1\n",
       "...  ..\n",
       "1995  1\n",
       "1996 -1\n",
       "1997 -1\n",
       "1998  1\n",
       "1999  1\n",
       "\n",
       "[2000 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    1000\n",
       " 1    1000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policzności etykiet stwierdzamy, że zbiór treningowy jest zbalansowany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>483</td>\n",
       "      <td>454</td>\n",
       "      <td>513</td>\n",
       "      <td>495</td>\n",
       "      <td>523</td>\n",
       "      <td>469</td>\n",
       "      <td>453</td>\n",
       "      <td>477</td>\n",
       "      <td>506</td>\n",
       "      <td>479</td>\n",
       "      <td>...</td>\n",
       "      <td>455</td>\n",
       "      <td>480</td>\n",
       "      <td>543</td>\n",
       "      <td>259</td>\n",
       "      <td>413</td>\n",
       "      <td>520</td>\n",
       "      <td>485</td>\n",
       "      <td>498</td>\n",
       "      <td>523</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>485</td>\n",
       "      <td>508</td>\n",
       "      <td>493</td>\n",
       "      <td>487</td>\n",
       "      <td>478</td>\n",
       "      <td>472</td>\n",
       "      <td>504</td>\n",
       "      <td>476</td>\n",
       "      <td>479</td>\n",
       "      <td>475</td>\n",
       "      <td>...</td>\n",
       "      <td>486</td>\n",
       "      <td>480</td>\n",
       "      <td>535</td>\n",
       "      <td>534</td>\n",
       "      <td>514</td>\n",
       "      <td>452</td>\n",
       "      <td>484</td>\n",
       "      <td>495</td>\n",
       "      <td>548</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483</td>\n",
       "      <td>521</td>\n",
       "      <td>507</td>\n",
       "      <td>475</td>\n",
       "      <td>493</td>\n",
       "      <td>486</td>\n",
       "      <td>421</td>\n",
       "      <td>475</td>\n",
       "      <td>496</td>\n",
       "      <td>483</td>\n",
       "      <td>...</td>\n",
       "      <td>491</td>\n",
       "      <td>476</td>\n",
       "      <td>498</td>\n",
       "      <td>495</td>\n",
       "      <td>508</td>\n",
       "      <td>528</td>\n",
       "      <td>486</td>\n",
       "      <td>465</td>\n",
       "      <td>508</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>474</td>\n",
       "      <td>504</td>\n",
       "      <td>576</td>\n",
       "      <td>480</td>\n",
       "      <td>553</td>\n",
       "      <td>483</td>\n",
       "      <td>524</td>\n",
       "      <td>478</td>\n",
       "      <td>483</td>\n",
       "      <td>483</td>\n",
       "      <td>...</td>\n",
       "      <td>521</td>\n",
       "      <td>475</td>\n",
       "      <td>470</td>\n",
       "      <td>463</td>\n",
       "      <td>509</td>\n",
       "      <td>525</td>\n",
       "      <td>479</td>\n",
       "      <td>467</td>\n",
       "      <td>552</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>495</td>\n",
       "      <td>474</td>\n",
       "      <td>523</td>\n",
       "      <td>479</td>\n",
       "      <td>495</td>\n",
       "      <td>488</td>\n",
       "      <td>485</td>\n",
       "      <td>476</td>\n",
       "      <td>497</td>\n",
       "      <td>478</td>\n",
       "      <td>...</td>\n",
       "      <td>510</td>\n",
       "      <td>471</td>\n",
       "      <td>522</td>\n",
       "      <td>343</td>\n",
       "      <td>509</td>\n",
       "      <td>520</td>\n",
       "      <td>475</td>\n",
       "      <td>493</td>\n",
       "      <td>506</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>493</td>\n",
       "      <td>458</td>\n",
       "      <td>503</td>\n",
       "      <td>478</td>\n",
       "      <td>517</td>\n",
       "      <td>479</td>\n",
       "      <td>472</td>\n",
       "      <td>478</td>\n",
       "      <td>444</td>\n",
       "      <td>477</td>\n",
       "      <td>...</td>\n",
       "      <td>469</td>\n",
       "      <td>475</td>\n",
       "      <td>485</td>\n",
       "      <td>443</td>\n",
       "      <td>517</td>\n",
       "      <td>486</td>\n",
       "      <td>474</td>\n",
       "      <td>489</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>481</td>\n",
       "      <td>484</td>\n",
       "      <td>481</td>\n",
       "      <td>490</td>\n",
       "      <td>449</td>\n",
       "      <td>481</td>\n",
       "      <td>467</td>\n",
       "      <td>478</td>\n",
       "      <td>469</td>\n",
       "      <td>483</td>\n",
       "      <td>...</td>\n",
       "      <td>506</td>\n",
       "      <td>485</td>\n",
       "      <td>508</td>\n",
       "      <td>599</td>\n",
       "      <td>498</td>\n",
       "      <td>527</td>\n",
       "      <td>481</td>\n",
       "      <td>490</td>\n",
       "      <td>455</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>485</td>\n",
       "      <td>485</td>\n",
       "      <td>530</td>\n",
       "      <td>480</td>\n",
       "      <td>444</td>\n",
       "      <td>487</td>\n",
       "      <td>462</td>\n",
       "      <td>475</td>\n",
       "      <td>509</td>\n",
       "      <td>494</td>\n",
       "      <td>...</td>\n",
       "      <td>442</td>\n",
       "      <td>474</td>\n",
       "      <td>502</td>\n",
       "      <td>368</td>\n",
       "      <td>453</td>\n",
       "      <td>482</td>\n",
       "      <td>478</td>\n",
       "      <td>481</td>\n",
       "      <td>484</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>477</td>\n",
       "      <td>469</td>\n",
       "      <td>528</td>\n",
       "      <td>485</td>\n",
       "      <td>483</td>\n",
       "      <td>469</td>\n",
       "      <td>482</td>\n",
       "      <td>477</td>\n",
       "      <td>494</td>\n",
       "      <td>476</td>\n",
       "      <td>...</td>\n",
       "      <td>473</td>\n",
       "      <td>476</td>\n",
       "      <td>453</td>\n",
       "      <td>638</td>\n",
       "      <td>471</td>\n",
       "      <td>538</td>\n",
       "      <td>470</td>\n",
       "      <td>490</td>\n",
       "      <td>613</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>482</td>\n",
       "      <td>453</td>\n",
       "      <td>515</td>\n",
       "      <td>481</td>\n",
       "      <td>500</td>\n",
       "      <td>493</td>\n",
       "      <td>503</td>\n",
       "      <td>477</td>\n",
       "      <td>501</td>\n",
       "      <td>475</td>\n",
       "      <td>...</td>\n",
       "      <td>484</td>\n",
       "      <td>478</td>\n",
       "      <td>487</td>\n",
       "      <td>694</td>\n",
       "      <td>493</td>\n",
       "      <td>499</td>\n",
       "      <td>474</td>\n",
       "      <td>494</td>\n",
       "      <td>536</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n",
       "0    483  454  513  495  523  469  453  477  506  479  ...  455  480  543   \n",
       "1    485  508  493  487  478  472  504  476  479  475  ...  486  480  535   \n",
       "2    483  521  507  475  493  486  421  475  496  483  ...  491  476  498   \n",
       "3    474  504  576  480  553  483  524  478  483  483  ...  521  475  470   \n",
       "4    495  474  523  479  495  488  485  476  497  478  ...  510  471  522   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "595  493  458  503  478  517  479  472  478  444  477  ...  469  475  485   \n",
       "596  481  484  481  490  449  481  467  478  469  483  ...  506  485  508   \n",
       "597  485  485  530  480  444  487  462  475  509  494  ...  442  474  502   \n",
       "598  477  469  528  485  483  469  482  477  494  476  ...  473  476  453   \n",
       "599  482  453  515  481  500  493  503  477  501  475  ...  484  478  487   \n",
       "\n",
       "     493  494  495  496  497  498  499  \n",
       "0    259  413  520  485  498  523  510  \n",
       "1    534  514  452  484  495  548  477  \n",
       "2    495  508  528  486  465  508  503  \n",
       "3    463  509  525  479  467  552  517  \n",
       "4    343  509  520  475  493  506  491  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  \n",
       "595  443  517  486  474  489  506  506  \n",
       "596  599  498  527  481  490  455  451  \n",
       "597  368  453  482  478  481  484  517  \n",
       "598  638  471  538  470  490  613  492  \n",
       "599  694  493  499  474  494  536  526  \n",
       "\n",
       "[600 rows x 500 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zbiór walidacyjny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelowanie ręczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iter, clf, params, Xtrain, ytrain,cv, selector=False):\n",
    "    random_search = RandomizedSearchCV(\n",
    "    clf,\n",
    "    param_distributions=params,\n",
    "    n_iter=iter,\n",
    "    scoring='balanced_accuracy',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    "    )\n",
    "    if selector:\n",
    "        selector = RFECV(estimator=clf, scoring='balanced_accuracy',verbose=1, min_features_to_select =400,importance_getter=\"feature_importances_\")\n",
    "        Xtrain = selector.fit_transform(Xtrain, ytrain)\n",
    "\n",
    "    random_search.fit(Xtrain, ytrain)\n",
    "    best_clf = random_search.best_estimator_\n",
    "\n",
    "    # Walidacja krzyżowa\n",
    "    scores = cross_val_score(best_clf, Xtrain, ytrain, cv=cv, scoring='balanced_accuracy')\n",
    "    \n",
    "    # ypred = best_clf.predict(Xtest)\n",
    "    # balanced_accuracy = balanced_accuracy_score(ytest, ypred)\n",
    "    return (best_clf, scores, selector) if selector else (best_clf, scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategia podziału dla walidacji krzyżowej, wspólna dla każdego modelu\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_DT = {\n",
    "    'max_depth': np.arange(1, 31), \n",
    "    'max_features': [None, 'log2', 'sqrt'],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'min_samples_split': np.arange(1, 61),  \n",
    "    'min_samples_leaf': np.arange(1, 61),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trening z walidacją krzyżową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n"
     ]
    }
   ],
   "source": [
    "best_clf, scores= train(1000, clf, params_DT, X,y,cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trening z rekukrencyjnym doborem cech i walidacją krzyżową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n"
     ]
    }
   ],
   "source": [
    "best_clf2, scores2, selector = train(1000, clf, params_DT, X,y,cv,selector=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7805000000000002"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7875000000000001"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybór klasyfikatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection improved the scores\n"
     ]
    }
   ],
   "source": [
    "if np.mean(scores2)>np.mean(scores):\n",
    "    with open('models/DT.pkl','wb') as f:\n",
    "        pickle.dump(best_clf2,f)\n",
    "    with open('selectors/DT.pkl','wb') as f:\n",
    "        pickle.dump(selector,f)\n",
    "    best_clf = best_clf2\n",
    "    X_test = selector.transform(Xtest)\n",
    "    print(\"Feature selection improved the scores\")\n",
    "else:\n",
    "    with open('models/DT.pkl','wb') as f:\n",
    "        pickle.dump(best_clf,f)\n",
    "    X_test = Xtest\n",
    "    print(\"All features used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_RF = {\n",
    "    'n_estimators': np.arange(1, 2001),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': np.arange(3, 11),\n",
    "    'min_samples_split': np.arange(2, 11),\n",
    "    'min_samples_leaf': np.arange(1, 11),\n",
    "    'bootstrap': [True, False],\n",
    "    'max_samples': np.arange(0, 1.1, 0.1),   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trening z walidacją krzyżową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    }
   ],
   "source": [
    "best_clf, scores = train(200, clf, params_RF, X,y,cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trening z rekukrencyjnym doborem cech i walidacją krzyżową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    }
   ],
   "source": [
    "best_clf2, scores2, selector = train(200, clf, params_RF, X,y,cv,selector=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6970000000000001"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6845"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybór klasyfikatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features used\n"
     ]
    }
   ],
   "source": [
    "if np.mean(scores2)>np.mean(scores):\n",
    "    with open('models/RF.pkl','wb') as f:\n",
    "        pickle.dump(best_clf2,f)\n",
    "    with open('selectors/RF.pkl','wb') as f:\n",
    "        pickle.dump(selector,f)\n",
    "    best_clf = best_clf2\n",
    "    X_test = selector.transform(Xtest)\n",
    "    print(\"Feature selection improved the scores\")\n",
    "else:\n",
    "    with open('models/RF.pkl','wb') as f:\n",
    "        pickle.dump(best_clf,f)\n",
    "    X_test = Xtest\n",
    "    print(\"All features used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(objective='binary:logistic', eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_XGB = {\n",
    "    'n_estimators': np.arange(50, 300, 50),\n",
    "    'max_depth': np.arange(3, 10),\n",
    "    'learning_rate': np.linspace(0.01, 0.2, 10),\n",
    "    'subsample': np.linspace(0.6, 1.0, 5),\n",
    "    'colsample_bytree': np.linspace(0.6, 1.0, 5),\n",
    "    'gamma': [0, 1, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trening z walidacją krzyżową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    }
   ],
   "source": [
    "best_clf, scores = train(200, clf, params_XGB, X,y.replace(-1,0),cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trening z rekukrencyjnym doborem cech i walidacją krzyżową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    }
   ],
   "source": [
    "best_clf2, scores2, selector = train(200, clf, params_XGB, X, y.replace(-1,0)  ,cv,selector=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8280000000000001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8330000000000002"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybór klasyfikatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection improved the scores\n"
     ]
    }
   ],
   "source": [
    "if np.mean(scores2)>np.mean(scores):\n",
    "    with open('models/XGB.pkl','wb') as f:\n",
    "        pickle.dump(best_clf2,f)\n",
    "    with open('selectors/XGB.pkl','wb') as f:\n",
    "        pickle.dump(selector,f)\n",
    "    X_test = selector.transform(Xtest)\n",
    "    print(\"Feature selection improved the scores\")\n",
    "else:\n",
    "    with open('models/XGB.pkl','wb') as f:\n",
    "        pickle.dump(best_clf,f)\n",
    "    X_test = selector.transform(Xtest)\n",
    "    print(\"All features used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predykcja prawodpodobieństw dla najlepszego klasyfikatora utworzonego ręcznie (bez doboru cech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = best_clf.predict_proba(X_test)\n",
    "np.savetxt('probs/313401_313392_artifical_model_prediction.txt', prob[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/XGB.pkl', 'rb') as f:\n",
    "    best_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanf1 = np.mean(cross_val_score(best_clf, X, y.replace(-1,0), cv=cv, scoring='f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8293715510767171"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podejście AutoML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby uniknąć problemów przy zautomatyzowanym tworzeniu klasyfikatora binarnego zamieniamy etykiety -1 na 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =y.replace(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.rename(columns={0: 'class'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, X_test, ytrain, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "full_data = pd.concat([X, y], axis=1)\n",
    "train_data = pd.concat([Xtrain, ytrain], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLJAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W celu zorientowania się w czasie treningu modelu i jakich conajmniej wyników można się spodziewać użyto `mode='Explain'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML directory: AutoML_3\n",
      "The task is binary_classification with evaluation metric f1\n",
      "AutoML will use algorithms: ['Baseline', 'Linear', 'Decision Tree', 'Random Forest', 'Xgboost', 'Neural Network']\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['simple_algorithms', 'default_algorithms', 'ensemble']\n",
      "* Step simple_algorithms will try to check up to 3 models\n",
      "1_Baseline f1 0.0 trained in 0.57 seconds\n",
      "2_DecisionTree f1 0.742616 trained in 41.58 seconds\n",
      "3_Linear f1 0.513347 trained in 19.08 seconds\n",
      "* Step default_algorithms will try to check up to 3 models\n",
      "4_Default_Xgboost f1 0.807229 trained in 38.0 seconds\n",
      "5_Default_NeuralNetwork f1 0.564202 trained in 12.22 seconds\n",
      "6_Default_RandomForest f1 0.792969 trained in 33.75 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble f1 0.812877 trained in 1.84 seconds\n",
      "AutoML fit time: 158.07 seconds\n",
      "AutoML best model: Ensemble\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AutoML(eval_metric=&#x27;f1&#x27;, ml_task=&#x27;binary_classification&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AutoML</label><div class=\"sk-toggleable__content\"><pre>AutoML(eval_metric=&#x27;f1&#x27;, ml_task=&#x27;binary_classification&#x27;, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AutoML(eval_metric='f1', ml_task='binary_classification', random_state=42)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl = AutoML(\n",
    "    ml_task=\"binary_classification\",\n",
    "    mode=\"Explain\",\n",
    "    eval_metric = \"f1\", \n",
    "    random_state=42\n",
    ")\n",
    "automl.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML directory: AutoML_2\n",
      "The task is binary_classification with evaluation metric f1\n",
      "AutoML will use algorithms: ['Decision Tree', 'Linear', 'Random Forest', 'Extra Trees', 'LightGBM', 'Xgboost', 'CatBoost', 'Neural Network', 'Nearest Neighbors']\n",
      "AutoML will stack models\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
      "* Step adjust_validation will try to check up to 1 model\n",
      "1_DecisionTree f1 0.628571 trained in 2.15 seconds\n",
      "Adjust validation. Remove: 1_DecisionTree\n",
      "*** Disable stacking for small dataset (nrows < 500)\n",
      "Validation strategy: 10-fold CV Shuffle,Stratify\n",
      "* Step simple_algorithms will try to check up to 4 models\n",
      "1_DecisionTree f1 0.524934 trained in 18.03 seconds\n",
      "2_DecisionTree f1 0.447368 trained in 16.5 seconds\n",
      "3_DecisionTree f1 0.469055 trained in 16.68 seconds\n",
      "4_Linear f1 0.46114 trained in 31.01 seconds\n",
      "* Step default_algorithms will try to check up to 6 models\n",
      "5_Default_LightGBM f1 0.631854 trained in 22.87 seconds\n",
      "6_Default_Xgboost f1 0.535809 trained in 28.38 seconds\n",
      "7_Default_CatBoost f1 0.637532 trained in 32.85 seconds\n",
      "8_Default_NeuralNetwork f1 0.486216 trained in 23.47 seconds\n",
      "9_Default_RandomForest f1 0.651042 trained in 39.88 seconds\n",
      "10_Default_ExtraTrees f1 0.681319 trained in 29.03 seconds\n",
      "* Step not_so_random will try to check up to 54 models\n",
      "20_LightGBM f1 0.614987 trained in 21.07 seconds\n",
      "11_Xgboost f1 0.551181 trained in 26.68 seconds\n",
      "29_CatBoost f1 0.653659 trained in 57.11 seconds\n",
      "38_RandomForest f1 0.637076 trained in 40.6 seconds\n",
      "47_ExtraTrees f1 0.649746 trained in 30.08 seconds\n",
      "56_NeuralNetwork f1 0.314685 trained in 24.1 seconds\n",
      "21_LightGBM f1 0.649077 trained in 23.03 seconds\n",
      "12_Xgboost f1 0.0 trained in 21.58 seconds\n",
      "30_CatBoost f1 0.681481 trained in 36.47 seconds\n",
      "39_RandomForest f1 0.64433 trained in 40.02 seconds\n",
      "48_ExtraTrees f1 0.665012 trained in 30.06 seconds\n",
      "57_NeuralNetwork f1 0.58547 trained in 25.81 seconds\n",
      "22_LightGBM f1 0.633766 trained in 23.23 seconds\n",
      "13_Xgboost f1 0.520833 trained in 30.36 seconds\n",
      "31_CatBoost f1 0.66313 trained in 131.51 seconds\n",
      "40_RandomForest f1 0.618454 trained in 36.2 seconds\n",
      "49_ExtraTrees f1 0.654912 trained in 31.2 seconds\n",
      "58_NeuralNetwork f1 0.451282 trained in 25.03 seconds\n",
      "23_LightGBM f1 0.637975 trained in 25.35 seconds\n",
      "14_Xgboost f1 0.475196 trained in 27.97 seconds\n",
      "32_CatBoost f1 0.636816 trained in 29.38 seconds\n",
      "41_RandomForest f1 0.647215 trained in 38.54 seconds\n",
      "50_ExtraTrees f1 0.648101 trained in 30.28 seconds\n",
      "59_NeuralNetwork f1 0.484536 trained in 25.65 seconds\n",
      "24_LightGBM f1 0.626866 trained in 24.4 seconds\n",
      "15_Xgboost f1 0.0 trained in 23.47 seconds\n",
      "33_CatBoost f1 0.654639 trained in 40.28 seconds\n",
      "42_RandomForest f1 0.625641 trained in 38.13 seconds\n",
      "51_ExtraTrees f1 0.652291 trained in 31.12 seconds\n",
      "60_NeuralNetwork f1 0.526316 trained in 26.55 seconds\n",
      "25_LightGBM f1 0.649616 trained in 25.03 seconds\n",
      "16_Xgboost f1 0.517333 trained in 30.53 seconds\n",
      "34_CatBoost f1 0.614583 trained in 32.28 seconds\n",
      "43_RandomForest f1 0.650246 trained in 45.9 seconds\n",
      "52_ExtraTrees f1 0.64467 trained in 32.97 seconds\n",
      "61_NeuralNetwork f1 0.486911 trained in 27.88 seconds\n",
      "26_LightGBM f1 0.653061 trained in 25.34 seconds\n",
      "17_Xgboost f1 0.487179 trained in 29.87 seconds\n",
      "35_CatBoost f1 0.626263 trained in 51.88 seconds\n",
      "44_RandomForest f1 0.631579 trained in 43.4 seconds\n",
      "53_ExtraTrees f1 0.678133 trained in 34.5 seconds\n",
      "62_NeuralNetwork f1 0.353698 trained in 27.74 seconds\n",
      "27_LightGBM f1 0.653061 trained in 27.12 seconds\n",
      "18_Xgboost f1 0.528497 trained in 34.18 seconds\n",
      "36_CatBoost f1 0.661538 trained in 73.87 seconds\n",
      "45_RandomForest f1 0.617284 trained in 39.43 seconds\n",
      "54_ExtraTrees f1 0.642674 trained in 34.7 seconds\n",
      "63_NeuralNetwork f1 0.333333 trained in 28.96 seconds\n",
      "28_LightGBM f1 0.625323 trained in 28.5 seconds\n",
      "19_Xgboost f1 0.481481 trained in 32.26 seconds\n",
      "37_CatBoost f1 0.624041 trained in 39.05 seconds\n",
      "46_RandomForest f1 0.638614 trained in 42.16 seconds\n",
      "55_ExtraTrees f1 0.645161 trained in 35.57 seconds\n",
      "64_NeuralNetwork f1 0.411594 trained in 29.43 seconds\n",
      "* Step golden_features will try to check up to 3 models\n",
      "None 50\n",
      "Add Golden Feature: 376_ratio_22\n",
      "Add Golden Feature: 22_ratio_376\n",
      "Add Golden Feature: 472_multiply_232\n",
      "Add Golden Feature: 263_diff_328\n",
      "Add Golden Feature: 439_multiply_130\n",
      "Add Golden Feature: 339_sum_80\n",
      "Add Golden Feature: 101_multiply_3\n",
      "Add Golden Feature: 232_multiply_38\n",
      "Add Golden Feature: 232_ratio_228\n",
      "Add Golden Feature: 228_ratio_232\n",
      "Add Golden Feature: 51_sum_10\n",
      "Add Golden Feature: 301_sum_93\n",
      "Add Golden Feature: 421_sum_327\n",
      "Add Golden Feature: 234_ratio_46\n",
      "Add Golden Feature: 214_diff_231\n",
      "Add Golden Feature: 228_diff_232\n",
      "Add Golden Feature: 77_ratio_124\n",
      "Add Golden Feature: 262_ratio_365\n",
      "Add Golden Feature: 456_multiply_292\n",
      "Add Golden Feature: 258_ratio_402\n",
      "Add Golden Feature: 402_ratio_258\n",
      "Add Golden Feature: 422_sum_326\n",
      "Add Golden Feature: 8_diff_230\n",
      "Add Golden Feature: 177_sum_105\n",
      "Add Golden Feature: 152_multiply_90\n",
      "Add Golden Feature: 152_sum_90\n",
      "Add Golden Feature: 10_diff_55\n",
      "Add Golden Feature: 26_ratio_4\n",
      "Add Golden Feature: 455_ratio_408\n",
      "Add Golden Feature: 355_ratio_498\n",
      "Add Golden Feature: 72_diff_396\n",
      "Add Golden Feature: 422_multiply_326\n",
      "Add Golden Feature: 335_sum_9\n",
      "Add Golden Feature: 7_diff_463\n",
      "Add Golden Feature: 90_diff_463\n",
      "Add Golden Feature: 387_diff_463\n",
      "Add Golden Feature: 90_ratio_463\n",
      "Add Golden Feature: 276_diff_463\n",
      "Add Golden Feature: 7_ratio_463\n",
      "Add Golden Feature: 261_diff_463\n",
      "Add Golden Feature: 276_ratio_463\n",
      "Add Golden Feature: 399_ratio_463\n",
      "Add Golden Feature: 261_ratio_463\n",
      "Add Golden Feature: 399_diff_463\n",
      "Add Golden Feature: 105_sum_85\n",
      "Add Golden Feature: 48_diff_311\n",
      "Add Golden Feature: 387_sum_364\n",
      "Add Golden Feature: 38_diff_263\n",
      "Add Golden Feature: 38_ratio_263\n",
      "Add Golden Feature: 263_ratio_38\n",
      "Created 50 Golden Features in 274.84 seconds.\n",
      "30_CatBoost_GoldenFeatures f1 0.65679 trained in 329.39 seconds\n",
      "* Step insert_random_feature will try to check up to 1 model\n",
      "30_CatBoost_RandomFeature f1 0.647959 trained in 197.72 seconds\n",
      "Drop features ['378', '472', '281', '475', '442', '451', '221', '318', '201', '41', '416', '234', '166', '310', '471', '332', '324', '95', '326', '457', '153', '170', '364', '60', '446', '423', '70', '66', '456', '366', '184', '452', '387', '249', '71', '395', '288', '151', '450', '42', '464', '122', '412', '389', '185', '431', '328', '120', '467', '270', '124', '34', '0', '327', '3', '335', '53', '50', '224', '343', '210', '373', '179', '19', '306', '69', '20', '61', '413', '430', '127', '374', '396', '304', '142', '174', '251', '2', '406', '499', '48', '8', '449', '372', '362', '130', '307', '359', '361', '46', '411', '469', '256', '231', '259', '237', '438', '241', '437', '410', '204', '345', '432', '218', '280', '92', '425', '429', '225', '195', '277', '337', '434', '123', '40', '244', '453', 'random_feature', '385', '441', '440', '44', '480', '87', '118', '132', '97', '329', '222', '139', '102', '51', '15', '405', '344', '403', '302', '116', '495', '297', '363', '143', '180', '129', '371', '36', '27', '56', '426', '158', '309', '491', '264', '175', '4', '275', '21', '247', '113', '207', '289', '171', '11', '488', '101', '108', '486', '300', '190', '419', '487', '33', '115', '109', '94', '424', '16', '73', '75', '161', '165', '74', '187', '100', '72', '197', '144', '68', '199', '137', '35', '76', '145', '213', '198', '246', '93', '214', '91', '226', '90', '57', '83', '183', '52', '150', '186', '146', '86', '32', '134', '200', '147', '121', '212', '138', '6', '154', '28', '25', '7', '24', '23', '12', '22', '13', '248', '31', '177', '211', '242', '103', '133', '1', '168', '125', '227', '202', '228', '230', '205', '45', '172', '208', '114', '157', '111', '252', '312', '383', '299', '458', '447', '445', '444', '436', '463', '465', '498', '497', '493', '492', '482', '478', '468', '393', '386', '287', '293', '296', '267', '341', '350', '348', '355', '268', '323', '356', '263', '301', '357', '260', '258', '266', '313', '311', '303', '384', '435', '382', '376', '398', '399', '402', '404', '407', '415', '422', '381', '369', '370', '340', '380', '400', '104', '173', '164', '351', '290', '191', '253', '358', '107', '169', '489', '388', '427', '18', '55', '470', '196', '401', '448', '38', '333', '325', '331', '10', '279', '367', '160', '9', '67', '238', '240', '63', '14', '390', '481', '418', '156', '26', '255', '476', '136', '409', '375', '274', '321', '466', '317', '349', '203', '485', '39', '189', '330', '315', '379', '37', '408', '79', '354', '421', '59', '17', '346', '294', '474', '254', '291', '96', '286', '473', '30', '347', '461', '29', '243', '414', '219', '233', '106', '206', '342', '392', '89', '182', '339', '217', '477', '77', '141', '271', '308', '78', '284', '239', '352', '58', '88', '128', '454', '273', '193', '368', '250', '276', '443', '282', '479', '229', '5', '365', '181', '391', '278', '176', '49', '163', '283', '417', '64', '285', '82', '295', '215', '126', '216', '394', '460', '298', '397', '235', '112', '188', '484', '85', '262', '84', '162', '192', '269', '320', '483', '65', '261', '490', '336', '245', '420', '360', '152', '135', '319', '377', '272', '47', '167', '81', '220', '353', '43', '110', '496', '80', '462', '265', '428', '433', '292', '159', '148', '99', '178', '305', '459', '334', '209', '257', '223', '322', '131', '117', '155', '119', '232', '140', '314', '494', '316', '194', '98', '236', '62', '54', '149', '439']\n",
      "* Step features_selection will try to check up to 6 models\n",
      "30_CatBoost_SelectedFeatures f1 0.699267 trained in 24.83 seconds\n",
      "* Step hill_climbing_1 will try to check up to 19 models\n",
      "65_CatBoost_SelectedFeatures f1 0.704715 trained in 25.28 seconds\n",
      "66_CatBoost f1 0.690909 trained in 44.53 seconds\n",
      "67_ExtraTrees f1 0.662983 trained in 35.85 seconds\n",
      "68_ExtraTrees f1 0.676617 trained in 35.83 seconds\n",
      "69_ExtraTrees f1 0.678049 trained in 35.82 seconds\n",
      "70_CatBoost f1 0.688608 trained in 149.26 seconds\n",
      "* Step hill_climbing_2 will try to check up to 30 models\n",
      "71_CatBoost_SelectedFeatures f1 0.715365 trained in 25.39 seconds\n",
      "72_CatBoost_SelectedFeatures f1 0.696296 trained in 26.01 seconds\n",
      "73_CatBoost_SelectedFeatures f1 0.715686 trained in 25.24 seconds\n",
      "74_CatBoost_SelectedFeatures f1 0.715686 trained in 26.55 seconds\n",
      "75_ExtraTrees f1 0.681934 trained in 34.38 seconds\n",
      "76_ExtraTrees f1 0.686016 trained in 37.3 seconds\n",
      "77_ExtraTrees f1 0.658228 trained in 38.8 seconds\n",
      "78_ExtraTrees f1 0.647215 trained in 37.56 seconds\n",
      "79_ExtraTrees f1 0.666667 trained in 39.22 seconds\n",
      "80_LightGBM f1 0.653061 trained in 30.73 seconds\n",
      "* Step boost_on_errors will try to check up to 1 model\n",
      "73_CatBoost_SelectedFeatures_BoostOnErrors f1 0.720988 trained in 26.5 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble f1 0.724566 trained in 11.72 seconds\n",
      "AutoML fit time: 3476.18 seconds\n",
      "AutoML best model: Ensemble\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AutoML(eval_metric=&#x27;f1&#x27;, ml_task=&#x27;binary_classification&#x27;, mode=&#x27;Compete&#x27;,\n",
       "       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AutoML</label><div class=\"sk-toggleable__content\"><pre>AutoML(eval_metric=&#x27;f1&#x27;, ml_task=&#x27;binary_classification&#x27;, mode=&#x27;Compete&#x27;,\n",
       "       random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AutoML(eval_metric='f1', ml_task='binary_classification', mode='Compete',\n",
       "       random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl = AutoML(\n",
    "    ml_task=\"binary_classification\",\n",
    "    mode=\"Compete\",\n",
    "    eval_metric = \"f1\", \n",
    "    random_state=42\n",
    ")\n",
    "automl.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = automl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.9198966408268734\n",
      "BA: 0.9222199994998874\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1: {f1_score(y_test, y_pred)}\")\n",
    "print(f\"BA: {balanced_accuracy_score(y_test,y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified Compete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML directory: AutoML_1\n",
      "The task is binary_classification with evaluation metric f1\n",
      "AutoML will use algorithms: ['CatBoost', 'Xgboost', 'LightGBM']\n",
      "AutoML will stack models\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'hill_climbing_3', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
      "* Step adjust_validation will try to check up to 1 model\n",
      "1_DecisionTree f1 0.628571 trained in 1.07 seconds\n",
      "Adjust validation. Remove: 1_DecisionTree\n",
      "*** Disable stacking for small dataset (nrows < 500)\n",
      "Validation strategy: 10-fold CV Shuffle,Stratify\n",
      "Skip simple_algorithms because no parameters were generated.\n",
      "* Step default_algorithms will try to check up to 3 models\n",
      "1_Default_LightGBM f1 0.647668 trained in 11.35 seconds\n",
      "2_Default_Xgboost f1 0.535809 trained in 18.1 seconds\n",
      "3_Default_CatBoost f1 0.684864 trained in 24.05 seconds\n",
      "* Step not_so_random will try to check up to 42 models\n",
      "18_LightGBM f1 0.614987 trained in 8.75 seconds\n",
      "4_Xgboost f1 0.551181 trained in 13.9 seconds\n",
      "32_CatBoost f1 0.653659 trained in 44.6 seconds\n",
      "19_LightGBM f1 0.649077 trained in 10.89 seconds\n",
      "5_Xgboost f1 0.0 trained in 8.51 seconds\n",
      "33_CatBoost f1 0.681481 trained in 24.14 seconds\n",
      "20_LightGBM f1 0.633766 trained in 10.45 seconds\n",
      "6_Xgboost f1 0.520833 trained in 17.18 seconds\n",
      "34_CatBoost f1 0.66313 trained in 128.38 seconds\n",
      "21_LightGBM f1 0.637975 trained in 12.21 seconds\n",
      "7_Xgboost f1 0.475196 trained in 14.95 seconds\n",
      "35_CatBoost f1 0.636816 trained in 17.47 seconds\n",
      "22_LightGBM f1 0.626866 trained in 10.87 seconds\n",
      "8_Xgboost f1 0.0 trained in 9.52 seconds\n",
      "36_CatBoost f1 0.654639 trained in 28.13 seconds\n",
      "23_LightGBM f1 0.649616 trained in 11.06 seconds\n",
      "9_Xgboost f1 0.517333 trained in 16.59 seconds\n",
      "37_CatBoost f1 0.614583 trained in 19.45 seconds\n",
      "24_LightGBM f1 0.653061 trained in 11.03 seconds\n",
      "10_Xgboost f1 0.487179 trained in 15.69 seconds\n",
      "38_CatBoost f1 0.626263 trained in 38.61 seconds\n",
      "25_LightGBM f1 0.653061 trained in 11.96 seconds\n",
      "11_Xgboost f1 0.528497 trained in 19.2 seconds\n",
      "39_CatBoost f1 0.661538 trained in 58.65 seconds\n",
      "26_LightGBM f1 0.625323 trained in 13.09 seconds\n",
      "12_Xgboost f1 0.481481 trained in 17.39 seconds\n",
      "40_CatBoost f1 0.624041 trained in 23.24 seconds\n",
      "27_LightGBM f1 0.651042 trained in 12.44 seconds\n",
      "13_Xgboost f1 0.490765 trained in 20.76 seconds\n",
      "41_CatBoost f1 0.670077 trained in 27.86 seconds\n",
      "28_LightGBM f1 0.596939 trained in 12.32 seconds\n",
      "14_Xgboost f1 0.496124 trained in 22.32 seconds\n",
      "42_CatBoost f1 0.642674 trained in 73.6 seconds\n",
      "29_LightGBM f1 0.608911 trained in 13.53 seconds\n",
      "15_Xgboost f1 0.0 trained in 11.76 seconds\n",
      "43_CatBoost f1 0.619423 trained in 47.98 seconds\n",
      "30_LightGBM f1 0.632979 trained in 13.03 seconds\n",
      "16_Xgboost f1 0.509284 trained in 25.4 seconds\n",
      "44_CatBoost f1 0.683673 trained in 73.51 seconds\n",
      "31_LightGBM f1 0.653061 trained in 12.74 seconds\n",
      "17_Xgboost f1 0.0 trained in 12.44 seconds\n",
      "45_CatBoost f1 0.643564 trained in 18.95 seconds\n",
      "* Step golden_features will try to check up to 3 models\n",
      "None 50\n",
      "Add Golden Feature: 376_ratio_22\n",
      "Add Golden Feature: 22_ratio_376\n",
      "Add Golden Feature: 472_multiply_232\n",
      "Add Golden Feature: 263_diff_328\n",
      "Add Golden Feature: 439_multiply_130\n",
      "Add Golden Feature: 339_sum_80\n",
      "Add Golden Feature: 101_multiply_3\n",
      "Add Golden Feature: 232_multiply_38\n",
      "Add Golden Feature: 232_ratio_228\n",
      "Add Golden Feature: 228_ratio_232\n",
      "Add Golden Feature: 51_sum_10\n",
      "Add Golden Feature: 301_sum_93\n",
      "Add Golden Feature: 421_sum_327\n",
      "Add Golden Feature: 234_ratio_46\n",
      "Add Golden Feature: 214_diff_231\n",
      "Add Golden Feature: 228_diff_232\n",
      "Add Golden Feature: 77_ratio_124\n",
      "Add Golden Feature: 262_ratio_365\n",
      "Add Golden Feature: 456_multiply_292\n",
      "Add Golden Feature: 258_ratio_402\n",
      "Add Golden Feature: 402_ratio_258\n",
      "Add Golden Feature: 422_sum_326\n",
      "Add Golden Feature: 8_diff_230\n",
      "Add Golden Feature: 177_sum_105\n",
      "Add Golden Feature: 152_multiply_90\n",
      "Add Golden Feature: 152_sum_90\n",
      "Add Golden Feature: 10_diff_55\n",
      "Add Golden Feature: 26_ratio_4\n",
      "Add Golden Feature: 455_ratio_408\n",
      "Add Golden Feature: 355_ratio_498\n",
      "Add Golden Feature: 72_diff_396\n",
      "Add Golden Feature: 422_multiply_326\n",
      "Add Golden Feature: 335_sum_9\n",
      "Add Golden Feature: 7_diff_463\n",
      "Add Golden Feature: 90_diff_463\n",
      "Add Golden Feature: 387_diff_463\n",
      "Add Golden Feature: 90_ratio_463\n",
      "Add Golden Feature: 276_diff_463\n",
      "Add Golden Feature: 7_ratio_463\n",
      "Add Golden Feature: 261_diff_463\n",
      "Add Golden Feature: 276_ratio_463\n",
      "Add Golden Feature: 399_ratio_463\n",
      "Add Golden Feature: 261_ratio_463\n",
      "Add Golden Feature: 399_diff_463\n",
      "Add Golden Feature: 105_sum_85\n",
      "Add Golden Feature: 48_diff_311\n",
      "Add Golden Feature: 387_sum_364\n",
      "Add Golden Feature: 38_diff_263\n",
      "Add Golden Feature: 38_ratio_263\n",
      "Add Golden Feature: 263_ratio_38\n",
      "Created 50 Golden Features in 255.24 seconds.\n",
      "3_Default_CatBoost_GoldenFeatures f1 0.658228 trained in 285.46 seconds\n",
      "44_CatBoost_GoldenFeatures f1 0.632391 trained in 76.55 seconds\n",
      "33_CatBoost_GoldenFeatures f1 0.65679 trained in 36.74 seconds\n",
      "* Step insert_random_feature will try to check up to 1 model\n",
      "3_Default_CatBoost_RandomFeature f1 0.6 trained in 179.26 seconds\n",
      "Drop features ['95', '128', '271', '127', '270', '204', '318', '378', '124', '442', '71', '221', '159', '296', '431', '220', '445', '120', '49', '359', '37', '106', '377', '360', '455', '493', '358', '232', '158', '326', '48', '418', '170', '475', '446', '458', '468', '105', '439', '472', '300', '43', '323', '229', '421', '191', '294', '24', '304', '130', '238', '259', '410', '65', '89', '441', '282', '147', '216', '234', '197', '314', '219', '289', '459', '133', '91', '465', '151', '41', '107', '129', '85', '226', '102', '413', '495', '178', '135', '322', '252', '480', '394', '208', '272', '123', '438', '411', '193', '430', '385', '162', '466', '342', '59', '75', '166', '434', '408', '453', '333', '92', '179', '368', '51', '152', '122', '154', '286', '319', '477', '492', '284', '5', '364', '182', '66', '31', '388', '487', '281', '246', '497', 'random_feature', '335', '186', '426', '171', '485', '417', '192', '239', '10', '176', '409', '317', '336', '242', '194', '303', '248', '406', '496', '268', '87', '50', '72', '101', '389', '340', '264', '260', '250', '79', '363', '126', '247', '402', '403', '76', '121', '404', '227', '405', '118', '169', '113', '240', '155', '78', '244', '168', '156', '97', '150', '84', '70', '243', '83', '143', '110', '74', '111', '163', '109', '44', '222', '61', '225', '58', '57', '52', '183', '47', '46', '45', '172', '203', '40', '39', '35', '137', '173', '175', '189', '188', '187', '134', '1', '4', '7', '8', '231', '207', '98', '209', '67', '131', '25', '212', '23', '21', '20', '19', '16', '217', '218', '15', '27', '14', '12', '11', '9', '2', '90', '261', '435', '258', '337', '452', '450', '316', '448', '444', '437', '436', '451', '263', '463', '357', '491', '489', '486', '467', '339', '476', '254', '473', '469', '460', '464', '433', '313', '276', '265', '275', '266', '267', '329', '279', '262', '412', '280', '312', '287', '311', '285', '309', '306', '305', '302', '301', '256', '295', '293', '482', '277', '432', '400', '346', '384', '428', '367', '356', '429', '348', '251', '399', '390', '349', '380', '376', '381', '365', '353', '371', '370', '387', '369', '396', '422', '424', '392', '414', '423', '419', '253', '425', '420', '274', '22', '161', '198', '132', '29', '148', '180', '479', '490', '96', '33', '398', '298', '139', '325', '160', '214', '167', '115', '308', '361', '199', '53', '77', '177', '157', '483', '447', '136', '373', '145', '324', '345', '481', '307', '233', '153', '206', '141', '355', '462', '164', '291', '395', '457', '328', '125', '372', '366', '344', '165', '471', '224', '347', '341', '454', '0', '299', '81', '416', '99', '190', '213', '391', '440', '146', '3', '484', '255', '202', '80', '330', '331', '149', '116', '362', '64', '354', '321', '210', '88', '245', '55', '315', '456', '443', '54', '292', '237', '427', '119', '93', '82', '236', '185', '383', '56', '68', '397', '196', '200', '334', '449', '142', '30', '310', '374', '393', '407', '269', '470', '401', '343', '62', '69', '386', '26', '13', '352', '114', '494', '17', '290', '104', '332', '94', '350', '228', '235', '100', '249', '211', '117', '103', '382', '297', '230', '201', '498', '60', '140', '42', '6', '375', '112', '86', '205', '73', '327', '28', '34', '138', '108', '257', '283', '38', '223', '36', '273', '461', '144', '474', '488', '215', '32', '288', '181', '195', '174', '379', '415', '63', '351', '18', '478', '499']\n",
      "* Step features_selection will try to check up to 3 models\n",
      "3_Default_CatBoost_SelectedFeatures f1 0.702564 trained in 11.02 seconds\n",
      "31_LightGBM_SelectedFeatures f1 0.696809 trained in 14.74 seconds\n",
      "4_Xgboost_SelectedFeatures f1 0.601583 trained in 15.61 seconds\n",
      "* Step hill_climbing_1 will try to check up to 22 models\n",
      "46_CatBoost_SelectedFeatures f1 0.70557 trained in 11.43 seconds\n",
      "47_LightGBM_SelectedFeatures f1 0.66129 trained in 15.14 seconds\n",
      "48_LightGBM_SelectedFeatures f1 0.694087 trained in 13.61 seconds\n",
      "49_CatBoost f1 0.657963 trained in 29.12 seconds\n",
      "50_CatBoost f1 0.666667 trained in 59.48 seconds\n",
      "51_CatBoost f1 0.646739 trained in 76.89 seconds\n",
      "52_CatBoost f1 0.637755 trained in 27.46 seconds\n",
      "53_CatBoost f1 0.643038 trained in 30.08 seconds\n",
      "54_LightGBM f1 0.663185 trained in 16.58 seconds\n",
      "55_LightGBM f1 0.625323 trained in 17.08 seconds\n",
      "56_LightGBM f1 0.63807 trained in 15.23 seconds\n",
      "57_LightGBM f1 0.65995 trained in 15.05 seconds\n",
      "58_LightGBM f1 0.63807 trained in 15.35 seconds\n",
      "59_LightGBM f1 0.65995 trained in 15.15 seconds\n",
      "60_Xgboost_SelectedFeatures f1 0.615385 trained in 17.39 seconds\n",
      "61_Xgboost_SelectedFeatures f1 0.615789 trained in 17.2 seconds\n",
      "62_Xgboost f1 0.530928 trained in 21.58 seconds\n",
      "63_Xgboost f1 0.535065 trained in 21.25 seconds\n",
      "64_Xgboost f1 0.519685 trained in 24.34 seconds\n",
      "65_Xgboost f1 0.544503 trained in 26.6 seconds\n",
      "66_Xgboost f1 0.516624 trained in 23.26 seconds\n",
      "67_Xgboost f1 0.515152 trained in 23.87 seconds\n",
      "* Step hill_climbing_2 will try to check up to 24 models\n",
      "68_CatBoost_SelectedFeatures f1 0.69496 trained in 13.48 seconds\n",
      "69_CatBoost_SelectedFeatures f1 0.708661 trained in 14.15 seconds\n",
      "70_CatBoost_SelectedFeatures f1 0.69589 trained in 13.22 seconds\n",
      "71_CatBoost_SelectedFeatures f1 0.713911 trained in 14.43 seconds\n",
      "72_LightGBM_SelectedFeatures f1 0.696809 trained in 17.5 seconds\n",
      "73_LightGBM_SelectedFeatures f1 0.696809 trained in 17.63 seconds\n",
      "74_LightGBM_SelectedFeatures f1 0.694087 trained in 16.83 seconds\n",
      "75_LightGBM_SelectedFeatures f1 0.694087 trained in 16.67 seconds\n",
      "76_CatBoost f1 0.647668 trained in 23.38 seconds\n",
      "77_CatBoost f1 0.648101 trained in 41.6 seconds\n",
      "78_CatBoost f1 0.640827 trained in 50.1 seconds\n",
      "79_CatBoost f1 0.647215 trained in 118.31 seconds\n",
      "80_LightGBM f1 0.663185 trained in 19.41 seconds\n",
      "81_LightGBM f1 0.663185 trained in 19.35 seconds\n",
      "82_LightGBM_SelectedFeatures f1 0.66129 trained in 18.75 seconds\n",
      "83_LightGBM_SelectedFeatures f1 0.66129 trained in 19.08 seconds\n",
      "84_Xgboost_SelectedFeatures f1 0.640827 trained in 20.75 seconds\n",
      "85_Xgboost_SelectedFeatures f1 0.623037 trained in 20.42 seconds\n",
      "86_Xgboost_SelectedFeatures f1 0.626632 trained in 21.14 seconds\n",
      "87_Xgboost_SelectedFeatures f1 0.60733 trained in 20.7 seconds\n",
      "88_Xgboost_SelectedFeatures f1 0.616188 trained in 20.47 seconds\n",
      "89_Xgboost_SelectedFeatures f1 0.611825 trained in 20.42 seconds\n",
      "90_Xgboost f1 0.537859 trained in 25.42 seconds\n",
      "91_Xgboost f1 0.545455 trained in 23.3 seconds\n",
      "* Step hill_climbing_3 will try to check up to 5 models\n",
      "92_CatBoost_SelectedFeatures f1 0.73107 trained in 17.6 seconds\n",
      "93_CatBoost_SelectedFeatures f1 0.715789 trained in 17.48 seconds\n",
      "94_Xgboost_SelectedFeatures f1 0.626943 trained in 22.3 seconds\n",
      "95_Xgboost_SelectedFeatures f1 0.62963 trained in 22.05 seconds\n",
      "96_Xgboost_SelectedFeatures f1 0.633766 trained in 20.95 seconds\n",
      "* Step boost_on_errors will try to check up to 1 model\n",
      "92_CatBoost_SelectedFeatures_BoostOnErrors f1 0.723404 trained in 18.52 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble f1 0.737127 trained in 17.03 seconds\n",
      "AutoML fit time: 3094.45 seconds\n",
      "AutoML best model: Ensemble\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AutoML(algorithms=[&#x27;CatBoost&#x27;, &#x27;Xgboost&#x27;, &#x27;LightGBM&#x27;], eval_metric=&#x27;f1&#x27;,\n",
       "       hill_climbing_steps=3, ml_task=&#x27;binary_classification&#x27;, mode=&#x27;Compete&#x27;,\n",
       "       random_state=42, start_random_models=15, top_models_to_improve=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AutoML</label><div class=\"sk-toggleable__content\"><pre>AutoML(algorithms=[&#x27;CatBoost&#x27;, &#x27;Xgboost&#x27;, &#x27;LightGBM&#x27;], eval_metric=&#x27;f1&#x27;,\n",
       "       hill_climbing_steps=3, ml_task=&#x27;binary_classification&#x27;, mode=&#x27;Compete&#x27;,\n",
       "       random_state=42, start_random_models=15, top_models_to_improve=4)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AutoML(algorithms=['CatBoost', 'Xgboost', 'LightGBM'], eval_metric='f1',\n",
       "       hill_climbing_steps=3, ml_task='binary_classification', mode='Compete',\n",
       "       random_state=42, start_random_models=15, top_models_to_improve=4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl2 = AutoML(\n",
    "    algorithms=[\"CatBoost\", \"Xgboost\", \"LightGBM\"],\n",
    "    ml_task=\"binary_classification\",\n",
    "    start_random_models=15, \n",
    "    hill_climbing_steps= 3,\n",
    "    top_models_to_improve =4,\n",
    "    eval_metric = \"f1\",\n",
    "    mode=\"Compete\",\n",
    "    random_state=42\n",
    ")\n",
    "automl2.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = automl2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.9924433249370278\n",
      "BA: 0.9926108374384237\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1: {f1_score(y_test, y_pred)}\")\n",
    "print(f\"BA: {balanced_accuracy_score(y_test,y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wytrenujemy jeszcze modele na pełnych danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML directory: AutoML_2\n",
      "The task is binary_classification with evaluation metric f1\n",
      "AutoML will use algorithms: ['Decision Tree', 'Linear', 'Random Forest', 'Extra Trees', 'LightGBM', 'Xgboost', 'CatBoost', 'Neural Network', 'Nearest Neighbors']\n",
      "AutoML will stack models\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
      "* Step adjust_validation will try to check up to 1 model\n",
      "1_DecisionTree f1 0.736842 trained in 4.19 seconds\n",
      "Adjust validation. Remove: 1_DecisionTree\n",
      "Validation strategy: 5-fold CV Shuffle,Stratify\n",
      "* Step simple_algorithms will try to check up to 4 models\n",
      "1_DecisionTree f1 0.748707 trained in 6.66 seconds\n",
      "2_DecisionTree f1 0.610116 trained in 4.99 seconds\n",
      "3_DecisionTree f1 0.609795 trained in 4.76 seconds\n",
      "4_Linear f1 0.540247 trained in 18.77 seconds\n",
      "* Step default_algorithms will try to check up to 6 models\n",
      "5_Default_LightGBM f1 0.827246 trained in 17.31 seconds\n",
      "6_Default_Xgboost f1 0.817635 trained in 21.17 seconds\n",
      "7_Default_CatBoost f1 0.849824 trained in 18.91 seconds\n",
      "8_Default_NeuralNetwork f1 0.546311 trained in 12.37 seconds\n",
      "9_Default_RandomForest f1 0.800399 trained in 52.43 seconds\n",
      "10_Default_ExtraTrees f1 0.740895 trained in 21.17 seconds\n",
      "* Step not_so_random will try to check up to 54 models\n",
      "20_LightGBM f1 0.815362 trained in 9.42 seconds\n",
      "11_Xgboost f1 0.805403 trained in 19.3 seconds\n",
      "29_CatBoost f1 0.856859 trained in 29.18 seconds\n",
      "38_RandomForest f1 0.803607 trained in 61.67 seconds\n",
      "47_ExtraTrees f1 0.818953 trained in 21.83 seconds\n",
      "56_NeuralNetwork f1 0.54157 trained in 11.68 seconds\n",
      "21_LightGBM f1 0.832673 trained in 9.59 seconds\n",
      "12_Xgboost f1 0.652715 trained in 16.17 seconds\n",
      "30_CatBoost f1 0.827381 trained in 22.67 seconds\n",
      "39_RandomForest f1 0.713843 trained in 34.69 seconds\n",
      "48_ExtraTrees f1 0.707267 trained in 20.2 seconds\n",
      "57_NeuralNetwork f1 0.599834 trained in 13.11 seconds\n",
      "22_LightGBM f1 0.827962 trained in 12.19 seconds\n",
      "13_Xgboost f1 0.788618 trained in 21.95 seconds\n",
      "31_CatBoost f1 0.86014 trained in 101.34 seconds\n",
      "40_RandomForest f1 0.725798 trained in 26.39 seconds\n",
      "49_ExtraTrees f1 0.717092 trained in 24.73 seconds\n",
      "58_NeuralNetwork f1 0.556711 trained in 11.67 seconds\n",
      "23_LightGBM f1 0.81714 trained in 16.48 seconds\n",
      "14_Xgboost f1 0.723404 trained in 17.18 seconds\n",
      "32_CatBoost f1 0.793035 trained in 14.63 seconds\n",
      "41_RandomForest f1 0.792319 trained in 40.67 seconds\n",
      "50_ExtraTrees f1 0.732497 trained in 23.18 seconds\n",
      "59_NeuralNetwork f1 0.577821 trained in 12.12 seconds\n",
      "24_LightGBM f1 0.801806 trained in 12.43 seconds\n",
      "15_Xgboost f1 0.623 trained in 16.88 seconds\n",
      "33_CatBoost f1 0.859113 trained in 22.77 seconds\n",
      "42_RandomForest f1 0.707566 trained in 27.07 seconds\n",
      "51_ExtraTrees f1 0.686489 trained in 23.26 seconds\n",
      "60_NeuralNetwork f1 0.522147 trained in 13.4 seconds\n",
      "25_LightGBM f1 0.825915 trained in 16.78 seconds\n",
      "16_Xgboost f1 0.772107 trained in 16.31 seconds\n",
      "34_CatBoost f1 0.824864 trained in 17.28 seconds\n",
      "43_RandomForest f1 0.809619 trained in 59.19 seconds\n",
      "52_ExtraTrees f1 0.80222 trained in 21.29 seconds\n",
      "61_NeuralNetwork f1 0.465517 trained in 15.63 seconds\n",
      "26_LightGBM f1 0.813492 trained in 12.81 seconds\n",
      "17_Xgboost f1 0.677728 trained in 15.42 seconds\n",
      "35_CatBoost f1 0.839482 trained in 24.22 seconds\n",
      "44_RandomForest f1 0.811463 trained in 57.42 seconds\n",
      "53_ExtraTrees f1 0.820383 trained in 25.62 seconds\n",
      "62_NeuralNetwork f1 0.514065 trained in 14.77 seconds\n",
      "27_LightGBM f1 0.826044 trained in 18.59 seconds\n",
      "18_Xgboost f1 0.755133 trained in 24.17 seconds\n",
      "36_CatBoost f1 0.856004 trained in 42.44 seconds\n",
      "45_RandomForest f1 0.719262 trained in 32.77 seconds\n",
      "54_ExtraTrees f1 0.697342 trained in 23.84 seconds\n",
      "63_NeuralNetwork f1 0.525027 trained in 15.8 seconds\n",
      "28_LightGBM f1 0.821482 trained in 19.08 seconds\n",
      "19_Xgboost f1 0.74578 trained in 21.03 seconds\n",
      "37_CatBoost f1 0.852475 trained in 21.47 seconds\n",
      "46_RandomForest f1 0.798802 trained in 31.32 seconds\n",
      "55_ExtraTrees f1 0.733897 trained in 27.91 seconds\n",
      "64_NeuralNetwork f1 0.575463 trained in 14.51 seconds\n",
      "* Step golden_features will try to check up to 3 models\n",
      "None 50\n",
      "Add Golden Feature: 338_diff_378\n",
      "Add Golden Feature: 311_sum_241\n",
      "Add Golden Feature: 492_sum_442\n",
      "Add Golden Feature: 141_sum_105\n",
      "Add Golden Feature: 442_multiply_284\n",
      "Add Golden Feature: 69_diff_442\n",
      "Add Golden Feature: 43_ratio_105\n",
      "Add Golden Feature: 105_diff_469\n",
      "Add Golden Feature: 289_sum_241\n",
      "Add Golden Feature: 372_ratio_105\n",
      "Add Golden Feature: 138_diff_442\n",
      "Add Golden Feature: 105_ratio_372\n",
      "Add Golden Feature: 148_ratio_442\n",
      "Add Golden Feature: 442_ratio_148\n",
      "Add Golden Feature: 338_diff_355\n",
      "Add Golden Feature: 338_multiply_151\n",
      "Add Golden Feature: 105_diff_320\n",
      "Add Golden Feature: 177_ratio_105\n",
      "Add Golden Feature: 442_sum_316\n",
      "Add Golden Feature: 105_multiply_67\n",
      "Add Golden Feature: 425_ratio_105\n",
      "Add Golden Feature: 442_sum_177\n",
      "Add Golden Feature: 338_ratio_291\n",
      "Add Golden Feature: 291_ratio_338\n",
      "Add Golden Feature: 175_ratio_105\n",
      "Add Golden Feature: 442_sum_392\n",
      "Add Golden Feature: 237_diff_442\n",
      "Add Golden Feature: 284_ratio_105\n",
      "Add Golden Feature: 442_sum_387\n",
      "Add Golden Feature: 442_ratio_247\n",
      "Add Golden Feature: 338_sum_124\n",
      "Add Golden Feature: 473_sum_442\n",
      "Add Golden Feature: 22_diff_442\n",
      "Add Golden Feature: 442_multiply_138\n",
      "Add Golden Feature: 281_sum_48\n",
      "Add Golden Feature: 292_diff_475\n",
      "Add Golden Feature: 2_diff_105\n",
      "Add Golden Feature: 232_ratio_105\n",
      "Add Golden Feature: 338_multiply_290\n",
      "Add Golden Feature: 439_ratio_105\n",
      "Add Golden Feature: 421_sum_338\n",
      "Add Golden Feature: 275_sum_241\n",
      "Add Golden Feature: 442_sum_228\n",
      "Add Golden Feature: 154_diff_442\n",
      "Add Golden Feature: 128_diff_473\n",
      "Add Golden Feature: 202_diff_442\n",
      "Add Golden Feature: 23_ratio_251\n",
      "Add Golden Feature: 251_ratio_23\n",
      "Add Golden Feature: 422_ratio_105\n",
      "Add Golden Feature: 105_ratio_439\n",
      "Created 50 Golden Features in 347.51 seconds.\n",
      "31_CatBoost_GoldenFeatures f1 0.853188 trained in 522.91 seconds\n",
      "* Step insert_random_feature will try to check up to 1 model\n",
      "31_CatBoost_RandomFeature f1 0.858571 trained in 189.99 seconds\n",
      "Drop features ['162', '434', '4', '406', '83', '441', '438', '365', '45', '38', '154', '88', '399', '273', '483', '327', '159', '108', '34', '343', '408', '264', '379', '150', '324', '395', '21', '467', '295', '495', '323', '271', '412', '266', '148', '333', '298', '116', '222', '498', '452', '410', '341', '46', '54', '358', '334', '484', '213', '126', '229', '251', '169', '393', '377', '175', '432', '179', '316', '156', '417', '218', '435', '305', '10', '376', '462', '104', '204', '203', '226', '124', '142', '227', '78', '198', '430', '8', '466', '6', '186', '49', '269', '349', '347', '206', '283', '276', '302', '259', '390', '25', '181', '362', '429', '321', '265', '262', '41', '233', '63', '402', '69', '70', '163', '425', '139', '211', '255', '24', '101', '99', '13', '2', '12', '82', '81', '9', '7', '127', '79', '71', '72', '5', '73', '74', '75', '77', '130', '15', '26', '107', '29', '65', '23', '114', '20', '18', '103', '106', '109', '30', '110', '113', '100', '246', '120', '121', '122', '17', '3', '183', '1', '223', '157', 'random_feature', '182', '214', '217', '219', '225', '14', '228', '231', '232', '234', '235', '236', '165', '144', '145', '164', '160', '149', '180', '155', '168', '170', '178', '177', '176', '141', '173', '171', '140', '239', '240', '212', '35', '32', '62', '61', '57', '56', '55', '53', '52', '47', '44', '43', '42', '39', '37', '193', '196', '208', '205', '202', '199', '209', '197', '195', '194', '192', '191', '188', '187', '184', '86', '85', '95', '87', '491', '444', '287', '288', '439', '290', '431', '463', '428', '465', '468', '497', '291', '312', '492', '490', '422', '489', '488', '487', '486', '485', '482', '479', '340', '478', '330', '473', '471', '308', '469', '286', '447', '448', '282', '309', '311', '307', '306', '304', '314', '354', '258', '300', '260', '299', '297', '263', '294', '296', '293', '303', '0', '274', '275', '460', '459', '277', '279', '458', '456', '454', '280', '449', '427', '470', '387', '353', '386', '351', '252', '254', '415', '414', '413', '411', '320', '409', '326', '357', '407', '352', '344', '403', '322', '325', '398', '396', '394', '392', '247', '242', '499', '93', '91', '90', '328', '256', '339', '367', '375', '381', '350', '374', '383', '373', '372', '371', '370', '368', '342', '337', '384', '366', '248', '363', '361', '388', '389', '332', '391', '421', '335', '284', '210', '94', '118', '19', '250', '268', '167', '58', '27', '317', '166', '67', '80', '190', '244', '102', '138', '220', '151', '461', '364', '418', '249', '360', '261', '66', '221', '215', '457', '132', '158', '419', '50', '315', '189', '436', '172', '112', '270', '117', '253', '496', '123', '480', '84', '494', '278', '40', '135', '201', '257', '96', '200', '125', '474', '33', '137', '161', '440', '60', '310', '238', '207', '369', '404', '313', '185', '272', '174', '147', '111', '385', '443', '423', '22', '89', '230', '97', '405', '400', '36', '98', '245', '397', '285', '134', '243', '292', '446', '216', '450', '289', '152', '345', '237', '401', '424', '131', '382', '426', '224', '143', '329', '416', '437', '136', '464', '11', '16', '380', '146', '420', '133', '59', '348', '76', '346']\n",
      "* Step features_selection will try to check up to 6 models\n",
      "31_CatBoost_SelectedFeatures f1 0.875434 trained in 18.97 seconds\n",
      "* Step hill_climbing_1 will try to check up to 21 models\n",
      "65_CatBoost_SelectedFeatures f1 0.872945 trained in 19.58 seconds\n",
      "66_CatBoost f1 0.86043 trained in 111.35 seconds\n",
      "67_CatBoost f1 0.854455 trained in 24.62 seconds\n",
      "68_LightGBM f1 0.832673 trained in 13.34 seconds\n",
      "69_LightGBM f1 0.827962 trained in 15.63 seconds\n",
      "70_LightGBM f1 0.827962 trained in 15.27 seconds\n",
      "71_LightGBM f1 0.827246 trained in 20.52 seconds\n",
      "72_LightGBM f1 0.827246 trained in 22.1 seconds\n",
      "73_ExtraTrees f1 0.816057 trained in 31.33 seconds\n",
      "* Step hill_climbing_2 will try to check up to 28 models\n",
      "74_CatBoost_SelectedFeatures f1 0.869608 trained in 13.94 seconds\n",
      "75_CatBoost_SelectedFeatures f1 0.873449 trained in 15.09 seconds\n",
      "76_CatBoost f1 0.860709 trained in 49.74 seconds\n",
      "77_LightGBM f1 0.832673 trained in 13.27 seconds\n",
      "78_LightGBM f1 0.827962 trained in 14.89 seconds\n",
      "79_ExtraTrees f1 0.801433 trained in 25.04 seconds\n",
      "80_ExtraTrees f1 0.828701 trained in 22.43 seconds\n",
      "81_ExtraTrees f1 0.814438 trained in 26.03 seconds\n",
      "82_ExtraTrees f1 0.828947 trained in 28.42 seconds\n",
      "83_Xgboost f1 0.808853 trained in 24.74 seconds\n",
      "84_ExtraTrees f1 0.809082 trained in 28.32 seconds\n",
      "* Step boost_on_errors will try to check up to 1 model\n",
      "31_CatBoost_SelectedFeatures_BoostOnErrors f1 0.869652 trained in 20.08 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble f1 0.876304 trained in 19.92 seconds\n",
      "* Step stack will try to check up to 59 models\n",
      "31_CatBoost_SelectedFeatures_Stacked f1 0.878412 trained in 39.76 seconds\n",
      "21_LightGBM_Stacked f1 0.876794 trained in 14.58 seconds\n",
      "82_ExtraTrees_Stacked f1 0.878848 trained in 25.09 seconds\n",
      "6_Default_Xgboost_Stacked f1 0.870904 trained in 28.29 seconds\n",
      "44_RandomForest_Stacked f1 0.876172 trained in 63.17 seconds\n",
      "57_NeuralNetwork_Stacked f1 0.745236 trained in 18.22 seconds\n",
      "75_CatBoost_SelectedFeatures_Stacked f1 0.877541 trained in 28.32 seconds\n",
      "68_LightGBM_Stacked f1 0.876794 trained in 14.83 seconds\n",
      "80_ExtraTrees_Stacked f1 0.875371 trained in 29.03 seconds\n",
      "83_Xgboost_Stacked f1 0.873142 trained in 25.82 seconds\n",
      "43_RandomForest_Stacked f1 0.879447 trained in 48.17 seconds\n",
      "59_NeuralNetwork_Stacked f1 0.782942 trained in 17.0 seconds\n",
      "65_CatBoost_SelectedFeatures_Stacked f1 0.880159 trained in 47.81 seconds\n",
      "77_LightGBM_Stacked f1 0.876794 trained in 15.51 seconds\n",
      "53_ExtraTrees_Stacked f1 0.876915 trained in 25.35 seconds\n",
      "11_Xgboost_Stacked f1 0.870728 trained in 19.97 seconds\n",
      "38_RandomForest_Stacked f1 0.875934 trained in 49.99 seconds\n",
      "64_NeuralNetwork_Stacked f1 0.726753 trained in 16.99 seconds\n",
      "74_CatBoost_SelectedFeatures_Stacked f1 0.877976 trained in 22.28 seconds\n",
      "22_LightGBM_Stacked f1 0.878412 trained in 17.73 seconds\n",
      "47_ExtraTrees_Stacked f1 0.87817 trained in 26.46 seconds\n",
      "13_Xgboost_Stacked f1 0.872277 trained in 21.63 seconds\n",
      "9_Default_RandomForest_Stacked not trained. Stop training after the first fold. Time needed to train on the first fold 6.0 seconds. The time estimate for training on all folds is larger than total_time_limit.\n",
      "58_NeuralNetwork_Stacked f1 0.76813 trained in 17.54 seconds\n",
      "76_CatBoost_Stacked not trained. Stop training after the first fold. Time needed to train on the first fold 8.0 seconds. The time estimate for training on all folds is larger than total_time_limit.\n",
      "* Step ensemble_stacked will try to check up to 1 model\n",
      "Ensemble_Stacked f1 0.880952 trained in 32.42 seconds\n",
      "AutoML fit time: 3651.25 seconds\n",
      "AutoML best model: Ensemble_Stacked\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AutoML(eval_metric=&#x27;f1&#x27;, ml_task=&#x27;binary_classification&#x27;, mode=&#x27;Compete&#x27;,\n",
       "       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AutoML</label><div class=\"sk-toggleable__content\"><pre>AutoML(eval_metric=&#x27;f1&#x27;, ml_task=&#x27;binary_classification&#x27;, mode=&#x27;Compete&#x27;,\n",
       "       random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AutoML(eval_metric='f1', ml_task='binary_classification', mode='Compete',\n",
       "       random_state=42)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "automl = AutoML(\n",
    "    ml_task=\"binary_classification\",\n",
    "    mode=\"Compete\",\n",
    "    eval_metric = \"f1\", \n",
    "    random_state=42\n",
    ")\n",
    "automl.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified Compete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML directory: AutoML_2\n",
      "The task is binary_classification with evaluation metric f1\n",
      "AutoML will use algorithms: ['CatBoost', 'Xgboost', 'LightGBM']\n",
      "AutoML will stack models\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'hill_climbing_3', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
      "* Step adjust_validation will try to check up to 1 model\n",
      "1_DecisionTree f1 0.736842 trained in 4.83 seconds\n",
      "Adjust validation. Remove: 1_DecisionTree\n",
      "Validation strategy: 5-fold CV Shuffle,Stratify\n",
      "Skip simple_algorithms because no parameters were generated.\n",
      "* Step default_algorithms will try to check up to 3 models\n",
      "1_Default_LightGBM f1 0.83045 trained in 29.23 seconds\n",
      "2_Default_Xgboost f1 0.817635 trained in 31.16 seconds\n",
      "3_Default_CatBoost f1 0.858416 trained in 24.65 seconds\n",
      "* Step not_so_random will try to check up to 42 models\n",
      "18_LightGBM f1 0.815362 trained in 17.65 seconds\n",
      "4_Xgboost f1 0.805403 trained in 28.11 seconds\n",
      "32_CatBoost f1 0.856859 trained in 37.88 seconds\n",
      "19_LightGBM f1 0.832673 trained in 19.57 seconds\n",
      "5_Xgboost f1 0.652715 trained in 26.32 seconds\n",
      "33_CatBoost f1 0.827381 trained in 33.23 seconds\n",
      "20_LightGBM f1 0.827962 trained in 21.04 seconds\n",
      "6_Xgboost f1 0.788618 trained in 29.04 seconds\n",
      "34_CatBoost f1 0.86014 trained in 105.57 seconds\n",
      "21_LightGBM f1 0.81714 trained in 26.69 seconds\n",
      "7_Xgboost f1 0.723404 trained in 26.96 seconds\n",
      "35_CatBoost f1 0.793035 trained in 23.68 seconds\n",
      "22_LightGBM f1 0.801806 trained in 22.3 seconds\n",
      "8_Xgboost f1 0.623 trained in 25.78 seconds\n",
      "36_CatBoost f1 0.859113 trained in 31.28 seconds\n",
      "23_LightGBM f1 0.825915 trained in 25.56 seconds\n",
      "9_Xgboost f1 0.772107 trained in 24.72 seconds\n",
      "37_CatBoost f1 0.824864 trained in 26.06 seconds\n",
      "24_LightGBM f1 0.813492 trained in 21.34 seconds\n",
      "10_Xgboost f1 0.677728 trained in 24.82 seconds\n",
      "38_CatBoost f1 0.839482 trained in 33.42 seconds\n",
      "25_LightGBM f1 0.826044 trained in 26.2 seconds\n",
      "11_Xgboost f1 0.755133 trained in 30.86 seconds\n",
      "39_CatBoost f1 0.856004 trained in 47.53 seconds\n",
      "26_LightGBM f1 0.821482 trained in 25.6 seconds\n",
      "12_Xgboost f1 0.74578 trained in 27.89 seconds\n",
      "40_CatBoost f1 0.852475 trained in 27.48 seconds\n",
      "27_LightGBM f1 0.818772 trained in 19.58 seconds\n",
      "13_Xgboost f1 0.731 trained in 32.37 seconds\n",
      "41_CatBoost f1 0.843844 trained in 27.99 seconds\n",
      "28_LightGBM f1 0.80995 trained in 19.49 seconds\n",
      "14_Xgboost f1 0.801401 trained in 30.02 seconds\n",
      "42_CatBoost f1 0.857711 trained in 59.35 seconds\n",
      "29_LightGBM f1 0.829755 trained in 26.93 seconds\n",
      "15_Xgboost f1 0.639113 trained in 23.73 seconds\n",
      "43_CatBoost f1 0.856004 trained in 40.17 seconds\n",
      "30_LightGBM f1 0.827215 trained in 20.82 seconds\n",
      "16_Xgboost f1 0.816592 trained in 40.62 seconds\n",
      "44_CatBoost f1 0.861386 trained in 56.59 seconds\n",
      "31_LightGBM f1 0.813492 trained in 20.52 seconds\n",
      "17_Xgboost f1 0.673857 trained in 27.24 seconds\n",
      "45_CatBoost f1 0.824814 trained in 26.85 seconds\n",
      "* Step golden_features will try to check up to 3 models\n",
      "None 50\n",
      "Add Golden Feature: 338_diff_378\n",
      "Add Golden Feature: 311_sum_241\n",
      "Add Golden Feature: 492_sum_442\n",
      "Add Golden Feature: 141_sum_105\n",
      "Add Golden Feature: 442_multiply_284\n",
      "Add Golden Feature: 69_diff_442\n",
      "Add Golden Feature: 43_ratio_105\n",
      "Add Golden Feature: 105_diff_469\n",
      "Add Golden Feature: 289_sum_241\n",
      "Add Golden Feature: 372_ratio_105\n",
      "Add Golden Feature: 138_diff_442\n",
      "Add Golden Feature: 105_ratio_372\n",
      "Add Golden Feature: 148_ratio_442\n",
      "Add Golden Feature: 442_ratio_148\n",
      "Add Golden Feature: 338_diff_355\n",
      "Add Golden Feature: 338_multiply_151\n",
      "Add Golden Feature: 105_diff_320\n",
      "Add Golden Feature: 177_ratio_105\n",
      "Add Golden Feature: 442_sum_316\n",
      "Add Golden Feature: 105_multiply_67\n",
      "Add Golden Feature: 425_ratio_105\n",
      "Add Golden Feature: 442_sum_177\n",
      "Add Golden Feature: 338_ratio_291\n",
      "Add Golden Feature: 291_ratio_338\n",
      "Add Golden Feature: 175_ratio_105\n",
      "Add Golden Feature: 442_sum_392\n",
      "Add Golden Feature: 237_diff_442\n",
      "Add Golden Feature: 284_ratio_105\n",
      "Add Golden Feature: 442_sum_387\n",
      "Add Golden Feature: 442_ratio_247\n",
      "Add Golden Feature: 338_sum_124\n",
      "Add Golden Feature: 473_sum_442\n",
      "Add Golden Feature: 22_diff_442\n",
      "Add Golden Feature: 442_multiply_138\n",
      "Add Golden Feature: 281_sum_48\n",
      "Add Golden Feature: 292_diff_475\n",
      "Add Golden Feature: 2_diff_105\n",
      "Add Golden Feature: 232_ratio_105\n",
      "Add Golden Feature: 338_multiply_290\n",
      "Add Golden Feature: 439_ratio_105\n",
      "Add Golden Feature: 421_sum_338\n",
      "Add Golden Feature: 275_sum_241\n",
      "Add Golden Feature: 442_sum_228\n",
      "Add Golden Feature: 154_diff_442\n",
      "Add Golden Feature: 128_diff_473\n",
      "Add Golden Feature: 202_diff_442\n",
      "Add Golden Feature: 23_ratio_251\n",
      "Add Golden Feature: 251_ratio_23\n",
      "Add Golden Feature: 422_ratio_105\n",
      "Add Golden Feature: 105_ratio_439\n",
      "Created 50 Golden Features in 342.37 seconds.\n",
      "44_CatBoost_GoldenFeatures f1 0.852018 trained in 415.12 seconds\n",
      "34_CatBoost_GoldenFeatures f1 0.853188 trained in 181.87 seconds\n",
      "* Step insert_random_feature will try to check up to 1 model\n",
      "44_CatBoost_RandomFeature f1 0.864625 trained in 160.88 seconds\n",
      "Drop features ['493', '310', '4', '231', '204', '286', '10', '173', '444', '282', '348', '277', '271', '278', '83', '329', '375', '389', '333', '258', '424', '74', '254', '296', '428', '134', '125', '151', '9', '293', '161', '177', '353', '193', '461', '305', '452', '55', '439', '327', '242', '260', '432', '308', '91', '468', '457', '217', '323', '127', '57', '476', '11', '195', '392', '111', '0', '364', '108', '247', '350', '448', '454', '209', '104', '427', '47', '483', '307', '19', 'random_feature', '469', '232', '5', '21', '228', '495', '61', '106', '44', '285', '12', '185', '456', '66', '43', '313', '49', '430', '445', '190', '168', '175', '349', '498', '17', '6', '261', '34', '399', '368', '410', '422', '18', '201', '301', '54', '354', '255', '213', '274', '437', '53', '409', '342', '492', '51', '450', '68', '130', '188', '416', '407', '114', '294', '337', '129', '279', '216', '413', '334', '345', '76', '309', '280', '415', '230', '152', '208', '446', '205', '246', '149', '42', '381', '385', '443', '222', '116', '346', '401', '178', '458', '107', '27', '315', '218', '221', '227', '372', '496', '72', '406', '373', '225', '70', '251', '88', '212', '473', '393', '82', '196', '131', '101', '113', '62', '497', '140', '150', '170', '491', '235', '124', '480', '186', '135', '197', '244', '466', '192', '467', '117', '477', '187', '478', '2', '162', '132', '488', '459', '215', '449', '460', '203', '109', '13', '219', '438', '233', '434', '100', '229', '440', '240', '102', '200', '441', '223', '169', '165', '486', '163', '210', '487', '174', '180', '183', '79', '78', '290', '67', '379', '328', '45', '330', '332', '295', '371', '380', '335', '288', '362', '52', '287', '400', '85', '344', '273', '351', '20', '403', '143', '253', '425', '97', '423', '22', '31', '404', '264', '266', '267', '90', '86', '248', '405', '283', '41', '395', '36', '390', '73', '387', '39', '388', '33', '245', '243', '121', '479', '179', '181', '397', '396', '382', '402', '357', '284', '418', '276', '352', '370', '99', '69', '40', '160', '122', '391', '239', '147', '436', '421', '322', '14', '148', '302', '474', '265', '142', '462', '489', '32', '172', '269', '16', '123', '207', '355', '408', '494', '298', '268', '359', '98', '485', '119', '26', '249', '304', '118', '289', '263', '75', '89', '182', '465', '95', '252', '319', '384', '226', '314', '419', '30', '71', '37', '470', '291', '103', '167', '250', '156', '136', '7', '325', '383', '343', '23', '367', '50', '206', '386', '238', '262', '202', '3', '292', '92', '189', '139', '138', '145', '35', '270', '146', '481', '236', '191', '29', '447', '339', '1', '63', '471', '320', '299', '374', '164', '414', '237', '490', '60', '81', '87', '394', '429', '65', '59', '199', '435', '166', '80', '331', '144', '426', '272', '321', '154', '25', '184', '8', '366', '155', '158', '38', '377', '312', '358', '126', '398', '46', '15', '220', '326', '24', '420', '159', '376', '115', '317', '194', '275', '361', '412', '112', '464', '499', '340', '157', '356', '257', '431', '211', '141', '56', '417', '347', '77', '311', '300', '303', '363', '224', '133', '482', '214', '259', '93', '137', '369', '484', '176', '96', '120', '110', '198', '94', '84', '341', '324', '297', '234']\n",
      "* Step features_selection will try to check up to 3 models\n",
      "44_CatBoost_SelectedFeatures f1 0.876427 trained in 21.59 seconds\n",
      "19_LightGBM_SelectedFeatures f1 0.852705 trained in 20.72 seconds\n",
      "2_Default_Xgboost_SelectedFeatures f1 0.839718 trained in 22.87 seconds\n",
      "* Step hill_climbing_1 will try to check up to 16 models\n",
      "46_CatBoost_SelectedFeatures f1 0.875374 trained in 23.41 seconds\n",
      "47_CatBoost f1 0.857571 trained in 58.23 seconds\n",
      "48_CatBoost f1 0.86043 trained in 118.58 seconds\n",
      "49_CatBoost f1 0.854455 trained in 33.73 seconds\n",
      "* Step hill_climbing_2 will try to check up to 19 models\n",
      "50_CatBoost_SelectedFeatures f1 0.870214 trained in 20.25 seconds\n",
      "51_CatBoost_SelectedFeatures f1 0.871514 trained in 26.03 seconds\n",
      "52_CatBoost_SelectedFeatures f1 0.870471 trained in 20.55 seconds\n",
      "53_CatBoost_SelectedFeatures f1 0.876685 trained in 29.61 seconds\n",
      "54_CatBoost f1 0.855283 trained in 47.21 seconds\n",
      "55_CatBoost f1 0.855166 trained in 102.54 seconds\n",
      "* Step hill_climbing_3 will try to check up to 12 models\n",
      "56_LightGBM_SelectedFeatures f1 0.865792 trained in 21.93 seconds\n",
      "57_Xgboost_SelectedFeatures f1 0.845961 trained in 24.67 seconds\n",
      "58_Xgboost_SelectedFeatures f1 0.841839 trained in 22.79 seconds\n",
      "59_LightGBM f1 0.841948 trained in 24.19 seconds\n",
      "60_LightGBM f1 0.83 trained in 25.08 seconds\n",
      "61_LightGBM f1 0.824465 trained in 32.79 seconds\n",
      "* Step boost_on_errors will try to check up to 1 model\n",
      "53_CatBoost_SelectedFeatures_BoostOnErrors f1 0.88004 trained in 34.46 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble f1 0.880597 trained in 11.07 seconds\n",
      "* Step stack will try to check up to 29 models\n",
      "53_CatBoost_SelectedFeatures_Stacked f1 0.881474 trained in 35.47 seconds\n",
      "56_LightGBM_SelectedFeatures_Stacked f1 0.872763 trained in 22.37 seconds\n",
      "57_Xgboost_SelectedFeatures_Stacked f1 0.864756 trained in 26.31 seconds\n",
      "44_CatBoost_SelectedFeatures_Stacked f1 0.881322 trained in 26.9 seconds\n",
      "19_LightGBM_SelectedFeatures_Stacked f1 0.87749 trained in 21.39 seconds\n",
      "58_Xgboost_SelectedFeatures_Stacked f1 0.870175 trained in 25.25 seconds\n",
      "46_CatBoost_SelectedFeatures_Stacked f1 0.883325 trained in 30.04 seconds\n",
      "59_LightGBM_Stacked f1 0.883791 trained in 28.1 seconds\n",
      "2_Default_Xgboost_SelectedFeatures_Stacked f1 0.871357 trained in 28.91 seconds\n",
      "51_CatBoost_SelectedFeatures_Stacked f1 0.881913 trained in 39.48 seconds\n",
      "19_LightGBM_Stacked f1 0.88253 trained in 24.9 seconds\n",
      "2_Default_Xgboost_Stacked f1 0.875063 trained in 36.51 seconds\n",
      "52_CatBoost_SelectedFeatures_Stacked f1 0.878728 trained in 23.99 seconds\n",
      "1_Default_LightGBM_Stacked f1 0.882559 trained in 31.37 seconds\n",
      "16_Xgboost_Stacked f1 0.876258 trained in 38.65 seconds\n",
      "50_CatBoost_SelectedFeatures_Stacked f1 0.881238 trained in 25.64 seconds\n",
      "60_LightGBM_Stacked f1 0.881035 trained in 27.9 seconds\n",
      "4_Xgboost_Stacked f1 0.876506 trained in 29.41 seconds\n",
      "44_CatBoost_Stacked f1 0.883813 trained in 58.88 seconds\n",
      "29_LightGBM_Stacked f1 0.883884 trained in 31.74 seconds\n",
      "14_Xgboost_Stacked not trained. Stop training after the first fold. Time needed to train on the first fold 3.0 seconds. The time estimate for training on all folds is larger than total_time_limit.\n",
      "48_CatBoost_Stacked not trained. Stop training after the first fold. Time needed to train on the first fold 20.0 seconds. The time estimate for training on all folds is larger than total_time_limit.\n",
      "* Step ensemble_stacked will try to check up to 1 model\n",
      "Ensemble_Stacked f1 0.887121 trained in 17.76 seconds\n",
      "AutoML fit time: 3644.49 seconds\n",
      "AutoML best model: Ensemble_Stacked\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AutoML(algorithms=[&#x27;CatBoost&#x27;, &#x27;Xgboost&#x27;, &#x27;LightGBM&#x27;], eval_metric=&#x27;f1&#x27;,\n",
       "       hill_climbing_steps=3, ml_task=&#x27;binary_classification&#x27;, mode=&#x27;Compete&#x27;,\n",
       "       random_state=42, start_random_models=15, top_models_to_improve=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AutoML</label><div class=\"sk-toggleable__content\"><pre>AutoML(algorithms=[&#x27;CatBoost&#x27;, &#x27;Xgboost&#x27;, &#x27;LightGBM&#x27;], eval_metric=&#x27;f1&#x27;,\n",
       "       hill_climbing_steps=3, ml_task=&#x27;binary_classification&#x27;, mode=&#x27;Compete&#x27;,\n",
       "       random_state=42, start_random_models=15, top_models_to_improve=4)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AutoML(algorithms=['CatBoost', 'Xgboost', 'LightGBM'], eval_metric='f1',\n",
       "       hill_climbing_steps=3, ml_task='binary_classification', mode='Compete',\n",
       "       random_state=42, start_random_models=15, top_models_to_improve=4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "automl2 = AutoML(\n",
    "    algorithms=[\"CatBoost\", \"Xgboost\", \"LightGBM\"],\n",
    "    ml_task=\"binary_classification\",\n",
    "    start_random_models=15, \n",
    "    hill_climbing_steps= 3,\n",
    "    top_models_to_improve =4,\n",
    "    eval_metric = \"f1\",\n",
    "    mode=\"Compete\",\n",
    "    random_state=42\n",
    ")\n",
    "automl2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba =automl2.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predykcja prawodpodobieństw dla najlepszego klasyfikatora utworzonego automatycznie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt('probs/313401_313392_artifical_automl_prediction.txt', proba[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autogluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównanie modelu AutoGluona z modelem MLJar wymaga wspólnej metryki - f1. Docelową metryką, na której testowany będzie najlepszy model jest balanced accuracy dostępny przy trenowaniu modelu AutoGluon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W celu zorientowania się w czasie treningu modelu i jakich conajmniej wyników można się spodziewać użyto `presets='medium_quality'`. Przyjmuje się, aby dla `presets='best_quality'`limit czasowy był co najmniej 16 razy dłuższym od czasu wykonania dla `presets='medium_quality'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240114_205615\"\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240114_205615\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.9.18\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.79 GB / 13.87 GB (34.5%)\n",
      "Disk Space Avail:   799.34 GB / 951.65 GB (84.0%)\n",
      "===================================================\n",
      "Train Data Rows:    1600\n",
      "Train Data Columns: 500\n",
      "Label Column:       class\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4901.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.10 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 500 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 500 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t500 features in original data used to generate 500 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.10 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'balanced_accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1280, Val Rows: 320\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.7036\t = Validation score   (balanced_accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.7036\t = Validation score   (balanced_accuracy)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.7595\t = Validation score   (balanced_accuracy)\n",
      "\t1.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.8001\t = Validation score   (balanced_accuracy)\n",
      "\t3.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.6505\t = Validation score   (balanced_accuracy)\n",
      "\t1.77s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.6783\t = Validation score   (balanced_accuracy)\n",
      "\t1.87s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.8314\t = Validation score   (balanced_accuracy)\n",
      "\t9.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.5907\t = Validation score   (balanced_accuracy)\n",
      "\t0.79s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.6096\t = Validation score   (balanced_accuracy)\n",
      "\t0.83s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "Metric balanced_accuracy is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 1: early stopping\n",
      "\t0.5877\t = Validation score   (balanced_accuracy)\n",
      "\t2.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.8003\t = Validation score   (balanced_accuracy)\n",
      "\t5.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.594\t = Validation score   (balanced_accuracy)\n",
      "\t2.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.8125\t = Validation score   (balanced_accuracy)\n",
      "\t10.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.68, 'LightGBMLarge': 0.12, 'RandomForestGini': 0.08, 'ExtraTreesEntr': 0.08, 'KNeighborsDist': 0.04}\n",
      "\t0.8407\t = Validation score   (balanced_accuracy)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 43.36s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240114_205615\")\n"
     ]
    }
   ],
   "source": [
    "startpredictor = TabularPredictor('class', eval_metric='balanced_accuracy').fit(train_data, time_limit=None, presets='medium_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'balanced_accuracy': 0.8098822235002876,\n",
       " 'accuracy': 0.81,\n",
       " 'mcc': 0.619888465000666,\n",
       " 'roc_auc': 0.8891250531369559,\n",
       " 'f1': 0.8061224489795918,\n",
       " 'precision': 0.8102564102564103,\n",
       " 'recall': 0.8020304568527918}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startpredictor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadanie zakończyło się w przeciągu 44 sekund. Ustalamy `time_limit = 60*60`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240114_210105\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 3600 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels\\ag-20240114_210105/ds_sub_fit/sub_fit_ho.\n",
      "Running the sub-fit in a ray process to avoid memory leakage.\n",
      "Spend 913 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 2687 seconds.\n",
      "Starting full fit now with num_stack_levels 0.\n",
      "Beginning AutoGluon training ... Time limit = 2687s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240114_210105\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.9.18\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.50 GB / 13.87 GB (32.5%)\n",
      "Disk Space Avail:   796.99 GB / 951.65 GB (83.7%)\n",
      "===================================================\n",
      "Train Data Rows:    1600\n",
      "Train Data Columns: 500\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4611.07 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.10 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 500 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 500 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t500 features in original data used to generate 500 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.10 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.87s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'balanced_accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 2686.13s of the 2686.12s of remaining time.\n",
      "\t0.7115\t = Validation score   (balanced_accuracy)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 2685.93s of the 2685.91s of remaining time.\n",
      "\t0.7115\t = Validation score   (balanced_accuracy)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2685.71s of the 2685.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.64%)\n",
      "\t0.7907\t = Validation score   (balanced_accuracy)\n",
      "\t10.38s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2669.01s of the 2669.0s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.72%)\n",
      "\t0.8225\t = Validation score   (balanced_accuracy)\n",
      "\t22.96s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 2640.02s of the 2640.0s of remaining time.\n",
      "\t0.6939\t = Validation score   (balanced_accuracy)\n",
      "\t1.97s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 2637.7s of the 2637.68s of remaining time.\n",
      "\t0.6957\t = Validation score   (balanced_accuracy)\n",
      "\t2.04s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2635.36s of the 2635.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.60%)\n",
      "\t0.8606\t = Validation score   (balanced_accuracy)\n",
      "\t86.63s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 2542.75s of the 2542.74s of remaining time.\n",
      "\t0.6438\t = Validation score   (balanced_accuracy)\n",
      "\t0.91s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 2541.5s of the 2541.48s of remaining time.\n",
      "\t0.6225\t = Validation score   (balanced_accuracy)\n",
      "\t0.95s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2540.23s of the 2540.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.39%)\n",
      "\t0.5756\t = Validation score   (balanced_accuracy)\n",
      "\t9.71s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 2524.52s of the 2524.5s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=9.60%)\n",
      "\t0.8338\t = Validation score   (balanced_accuracy)\n",
      "\t20.61s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 2497.38s of the 2497.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.84%)\n",
      "\t0.5921\t = Validation score   (balanced_accuracy)\n",
      "\t12.14s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2479.11s of the 2479.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=9.77%)\n",
      "\t0.8375\t = Validation score   (balanced_accuracy)\n",
      "\t81.84s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 2390.88s of the 2390.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=5.47%)\n",
      "\t0.8656\t = Validation score   (balanced_accuracy)\n",
      "\t67.2s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 2317.28s of the 2317.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.79%)\n",
      "\t0.6006\t = Validation score   (balanced_accuracy)\n",
      "\t14.12s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 2296.78s of the 2296.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.34%)\n",
      "\t0.8418\t = Validation score   (balanced_accuracy)\n",
      "\t25.0s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 2264.75s of the 2264.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.52%)\n",
      "\t0.5582\t = Validation score   (balanced_accuracy)\n",
      "\t19.83s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 2238.81s of the 2238.79s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 34.50% memory usage per fold, 69.00%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=34.50%)\n",
      "\t0.8026\t = Validation score   (balanced_accuracy)\n",
      "\t980.65s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 1253.87s of the 1253.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.82%)\n",
      "\t0.7276\t = Validation score   (balanced_accuracy)\n",
      "\t4.79s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 1242.98s of the 1242.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.79%)\n",
      "\t0.5944\t = Validation score   (balanced_accuracy)\n",
      "\t21.68s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 1215.16s of the 1215.15s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 33.55% memory usage per fold, 67.10%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=33.55%)\n",
      "\t0.8356\t = Validation score   (balanced_accuracy)\n",
      "\t111.66s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 1099.9s of the 1099.89s of remaining time.\n",
      "\t0.7795\t = Validation score   (balanced_accuracy)\n",
      "\t3.12s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 1096.45s of the 1096.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.78%)\n",
      "\t0.81\t = Validation score   (balanced_accuracy)\n",
      "\t41.73s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 1048.61s of the 1048.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.44%)\n",
      "\t0.5995\t = Validation score   (balanced_accuracy)\n",
      "\t27.16s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 1015.41s of the 1015.38s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.80% memory usage per fold, 75.21%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=18.80%)\n",
      "\t0.8675\t = Validation score   (balanced_accuracy)\n",
      "\t219.28s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 791.95s of the 791.94s of remaining time.\n",
      "\t0.8094\t = Validation score   (balanced_accuracy)\n",
      "\t20.5s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 771.11s of the 771.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=8.84%)\n",
      "\t0.807\t = Validation score   (balanced_accuracy)\n",
      "\t23.92s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 741.1s of the 741.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.40%)\n",
      "\t0.5279\t = Validation score   (balanced_accuracy)\n",
      "\t15.34s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 719.72s of the 719.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=9.17%)\n",
      "\t0.8419\t = Validation score   (balanced_accuracy)\n",
      "\t17.53s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 695.33s of the 695.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.77%)\n",
      "\t0.6283\t = Validation score   (balanced_accuracy)\n",
      "\t27.46s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 661.66s of the 661.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=6.53%)\n",
      "\t0.8262\t = Validation score   (balanced_accuracy)\n",
      "\t6.82s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 648.26s of the 648.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.89%)\n",
      "\t0.6076\t = Validation score   (balanced_accuracy)\n",
      "\t14.36s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 627.65s of the 627.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.96%)\n",
      "\t0.775\t = Validation score   (balanced_accuracy)\n",
      "\t77.7s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 543.45s of the 543.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.44%)\n",
      "\t0.5309\t = Validation score   (balanced_accuracy)\n",
      "\t23.71s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 513.69s of the 513.67s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.88% memory usage per fold, 71.52%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=17.88%)\n",
      "\t0.83\t = Validation score   (balanced_accuracy)\n",
      "\t38.27s\t = Training   runtime\n",
      "\t0.69s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 470.35s of the 470.33s of remaining time.\n",
      "\t0.8051\t = Validation score   (balanced_accuracy)\n",
      "\t2.92s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 467.11s of the 467.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.81%)\n",
      "\t0.8581\t = Validation score   (balanced_accuracy)\n",
      "\t65.86s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 394.71s of the 394.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.47%)\n",
      "\t0.5629\t = Validation score   (balanced_accuracy)\n",
      "\t11.9s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 376.86s of the 376.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.36%)\n",
      "\t0.5906\t = Validation score   (balanced_accuracy)\n",
      "\t7.96s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 362.27s of the 362.26s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 19.69% memory usage per fold, 78.77%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=19.69%)\n",
      "\t0.8518\t = Validation score   (balanced_accuracy)\n",
      "\t42.97s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 314.77s of the 314.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.43%)\n",
      "\t0.5861\t = Validation score   (balanced_accuracy)\n",
      "\t8.64s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 300.01s of the 300.0s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=9.16%)\n",
      "\t0.8088\t = Validation score   (balanced_accuracy)\n",
      "\t239.67s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 53.71s of the 53.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.65%)\n",
      "\t0.5713\t = Validation score   (balanced_accuracy)\n",
      "\t24.49s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 22.85s of the 22.84s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.02% memory usage per fold, 68.08%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=17.02%)\n",
      "\t0.7937\t = Validation score   (balanced_accuracy)\n",
      "\t20.9s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -3.46s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_r13_BAG_L1': 0.978, 'ExtraTreesEntr_BAG_L1': 0.022}\n",
      "\t0.8687\t = Validation score   (balanced_accuracy)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2691.93s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240114_210105\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor('class', eval_metric='balanced_accuracy').fit(train_data, time_limit=60*60, presets='best_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'balanced_accuracy': 0.835262934160186,\n",
       " 'accuracy': 0.835,\n",
       " 'mcc': 0.6706600439375775,\n",
       " 'roc_auc': 0.9059538396139131,\n",
       " 'f1': 0.835820895522388,\n",
       " 'precision': 0.8195121951219512,\n",
       " 'recall': 0.8527918781725888}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warto zauważyć, że otrzymane wyniki balanced_accuracy i f1 score zarówno przy `medium_quality` jak i `best_quality` są zbliżone. Wytrenujemy jeszcze model na pełnych danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240114_220432\"\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240114_220432\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.9.18\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.38 GB / 13.87 GB (31.6%)\n",
      "Disk Space Avail:   798.57 GB / 951.65 GB (83.9%)\n",
      "===================================================\n",
      "Train Data Rows:    2000\n",
      "Train Data Columns: 500\n",
      "Label Column:       class\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4467.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.63 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 500 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 500 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t500 features in original data used to generate 500 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.63 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'balanced_accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1600, Val Rows: 400\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.6825\t = Validation score   (balanced_accuracy)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.6825\t = Validation score   (balanced_accuracy)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.77\t = Validation score   (balanced_accuracy)\n",
      "\t1.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.835\t = Validation score   (balanced_accuracy)\n",
      "\t2.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.6825\t = Validation score   (balanced_accuracy)\n",
      "\t1.93s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.6975\t = Validation score   (balanced_accuracy)\n",
      "\t2.22s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.845\t = Validation score   (balanced_accuracy)\n",
      "\t12.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.6875\t = Validation score   (balanced_accuracy)\n",
      "\t0.88s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.6625\t = Validation score   (balanced_accuracy)\n",
      "\t0.84s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "Metric balanced_accuracy is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 1: early stopping\n",
      "\t0.5575\t = Validation score   (balanced_accuracy)\n",
      "\t2.5s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.8475\t = Validation score   (balanced_accuracy)\n",
      "\t4.99s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.5675\t = Validation score   (balanced_accuracy)\n",
      "\t4.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.83\t = Validation score   (balanced_accuracy)\n",
      "\t11.19s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'XGBoost': 0.333, 'LightGBMXT': 0.133, 'CatBoost': 0.133, 'ExtraTreesGini': 0.133, 'RandomForestGini': 0.067, 'RandomForestEntr': 0.067, 'LightGBM': 0.044, 'LightGBMLarge': 0.044, 'KNeighborsUnif': 0.022, 'NeuralNetFastAI': 0.022}\n",
      "\t0.86\t = Validation score   (balanced_accuracy)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 48.34s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240114_220432\")\n"
     ]
    }
   ],
   "source": [
    "startpredictor = TabularPredictor('class', eval_metric='balanced_accuracy').fit(full_data, time_limit=None, presets='medium_quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240114_220649\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 1920 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels\\ag-20240114_220649/ds_sub_fit/sub_fit_ho.\n",
      "Running the sub-fit in a ray process to avoid memory leakage.\n",
      "Spend 595 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 1325 seconds.\n",
      "Starting full fit now with num_stack_levels 0.\n",
      "Beginning AutoGluon training ... Time limit = 1325s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240114_220649\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.9.18\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.02 GB / 13.87 GB (29.0%)\n",
      "Disk Space Avail:   794.70 GB / 951.65 GB (83.5%)\n",
      "===================================================\n",
      "Train Data Rows:    2000\n",
      "Train Data Columns: 500\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4086.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.63 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 500 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 500 | ['0', '1', '2', '3', '4', ...]\n",
      "\t1.7s = Fit runtime\n",
      "\t500 features in original data used to generate 500 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.63 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'balanced_accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1323.23s of the 1323.19s of remaining time.\n",
      "\t0.7055\t = Validation score   (balanced_accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1322.88s of the 1322.86s of remaining time.\n",
      "\t0.7055\t = Validation score   (balanced_accuracy)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1322.52s of the 1322.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=3.62%)\n",
      "\t0.8125\t = Validation score   (balanced_accuracy)\n",
      "\t10.83s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1293.73s of the 1293.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=3.72%)\n",
      "\t0.8395\t = Validation score   (balanced_accuracy)\n",
      "\t45.22s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 1214.46s of the 1214.45s of remaining time.\n",
      "\t0.7075\t = Validation score   (balanced_accuracy)\n",
      "\t4.32s\t = Training   runtime\n",
      "\t1.39s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 1208.42s of the 1208.41s of remaining time.\n",
      "\t0.709\t = Validation score   (balanced_accuracy)\n",
      "\t4.38s\t = Training   runtime\n",
      "\t1.53s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1202.21s of the 1202.19s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=9.45%)\n",
      "\t0.867\t = Validation score   (balanced_accuracy)\n",
      "\t134.57s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 1031.22s of the 1031.2s of remaining time.\n",
      "\t0.609\t = Validation score   (balanced_accuracy)\n",
      "\t1.01s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 1029.78s of the 1029.76s of remaining time.\n",
      "\t0.638\t = Validation score   (balanced_accuracy)\n",
      "\t0.89s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1028.4s of the 1028.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.00%)\n",
      "\t0.5725\t = Validation score   (balanced_accuracy)\n",
      "\t11.06s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1005.66s of the 1005.64s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.16% memory usage per fold, 48.64%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=12.16%)\n",
      "\t0.84\t = Validation score   (balanced_accuracy)\n",
      "\t33.37s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 966.84s of the 966.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.11%)\n",
      "\t0.588\t = Validation score   (balanced_accuracy)\n",
      "\t15.98s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 944.26s of the 944.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=9.48%)\n",
      "\t0.845\t = Validation score   (balanced_accuracy)\n",
      "\t85.91s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 851.62s of the 851.6s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=5.48%)\n",
      "\t0.8645\t = Validation score   (balanced_accuracy)\n",
      "\t77.76s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 767.26s of the 767.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.01%)\n",
      "\t0.625\t = Validation score   (balanced_accuracy)\n",
      "\t17.4s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 743.34s of the 743.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=5.00%)\n",
      "\t0.852\t = Validation score   (balanced_accuracy)\n",
      "\t30.24s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 706.45s of the 706.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.91%)\n",
      "\t0.5455\t = Validation score   (balanced_accuracy)\n",
      "\t23.83s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 676.16s of the 676.14s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 31.42% memory usage per fold, 62.85%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=31.42%)\n",
      "\t0.82\t = Validation score   (balanced_accuracy)\n",
      "\t549.64s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 121.82s of the 121.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.25%)\n",
      "\t0.7565\t = Validation score   (balanced_accuracy)\n",
      "\t5.99s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 109.66s of the 109.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.04%)\n",
      "\t0.599\t = Validation score   (balanced_accuracy)\n",
      "\t23.13s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 80.26s of the 80.24s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 34.42% memory usage per fold, 68.84%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=34.42%)\n",
      "\t0.848\t = Validation score   (balanced_accuracy)\n",
      "\t75.15s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 0.9s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 1.0}\n",
      "\t0.867\t = Validation score   (balanced_accuracy)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1325.92s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240114_220649\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor('class', eval_metric='balanced_accuracy').fit(full_data, time_limit=32*60, presets='best_quality')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
